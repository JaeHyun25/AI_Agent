# LLM Orchestrator Agent v2.1 - Multi-Step & Self-Correcting
# 07-01 09:34 db ì¿¼ë¦¬ í–ˆì„ ë•Œ ê²°ê³¼ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (ë™ì  ë¼ìš°íŒ… ì„¤ê³„ ì¶”ê°€)
# 07-02 13:05 memory_retieverê°€ ì§ˆë¬¸-ë‹µë§Œ ê¸°ì–µí–ˆëŠ”ë°, ì¿¼ë¦¬, ë°ì´í„° ê²°ê³¼ì˜ ê³¼ì •ë„ ê¸°ì–µí•  ìˆ˜ ìˆê²Œ ìˆ˜ì •
# '''07-03 final ì½”ë“œê°€ ì˜ ë™ì‘í•œë‹¤. í•˜ì§€ë§Œ ê°€ë” conversation_historyì— ì§‘ì¤‘í•´ì„œ ë§í•˜ì§€ì•Šì•˜ëŠ”ë° ì§ˆë¬¸ê³¼ ìƒê´€ì—†ì´ ì´ì „ ëŒ€í™”ì— ëŒ€í•œ ê²ƒì„ ì°¾ì„ë•Œê°€ ìˆë‹¤.
#ê·¸ë˜ì„œ ê·¸ ë¶€ë¶„ì— ëŒ€í•´ ìˆ˜ì •ì„ ì‹œì‘í•œê²Œ 0704ë²„ì „ì´ë‹¤.''' ë”í•´ì„œ í† í° ì¸¡ì •í•˜ëŠ” ì½”ë“œë„ ì²¨ë¶€í–ˆë‹¤.
# 07-04 11:08 í”Œë˜ë„ˆê°€ ê¸°ì¡´ ëŒ€í™”ë¥¼ ìƒê°í•´ì„œ ì¢‹ì€ ê³„íš ì„¸ìš°ê²Œ í•˜ê¸° ì¶”ê°€
# 07-04 13:00 ê¸°ì–µì˜ ì „ë‹¬ ì‹¤íŒ¨ì— ëŒ€í•œ ìˆ˜ì •(answer_synthesizer) -> ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ëˆ„ì  ê¸°ëŠ¥(run_ai_agent)
# 07-04 17:21 í˜„ì¬ ë§ì´ ìˆ˜ì •í–ˆê³  ì´ ë²„ì „ì€ sql í”„ë¡¬í”„íŠ¸ë§Œ ìˆ˜ì •í•´ë´„. í˜„ì¬ 0704_2ë„ ì„±ëŠ¥ ì¢‹ìŒ...
# report, capacity agent ì¶”ê°€ ì¤‘(07-06 13:00)
# 0707ì€ temperature ì„¤ì •í–ˆëŠ”ë°, ì•ˆí•œ ê¸°ë³¸ ë²„ì „ì¸ 0706ì´ ë‚˜ì€ê±°ê°™ì•„ ì´ì–´ì„œ 0708ë¡œ...ì´ë²ˆì—” í•«ìŠ¤íŒŸ ê´€ë ¨ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
# 0708_2ëŠ” anomaly í•¨ìˆ˜ì™€ íˆ´ì— ê´€í•´ìˆ˜ì •.
import os
import io
import re
import sys
import json
import warnings
import logging
from datetime import datetime, timedelta
from contextlib import redirect_stdout
from typing import Optional, List, Dict

# RAGë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
import chromadb

# Third-party libraries
import matplotlib.pyplot as plt
import pandas as pd
import pymysql
import numpy as np

# 2025-06-25 ì¶”ê°€ pandasì˜ ëª¨ë“  float ì¶œë ¥ í˜•ì‹ì„ ì†Œìˆ˜ì  í•œ ìë¦¬ë¡œ ì„¤ì •
pd.options.display.float_format = '{:.1f}'.format
# --- 0. ë¡œê¹… ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼) ---
def setup_file_logger():
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    log_filename = os.path.join(log_dir, f"agent_log_{datetime.now().strftime('%Y-%m-%d')}.log")
    file_logger = logging.getLogger('FileLogger')
    file_logger.setLevel(logging.INFO)
    if not file_logger.handlers:    # í•¸ë“¤ëŸ¬ëŠ” ë¡œê·¸ ë©”ì‹œì§€ë¥¼ íŠ¹ì • ëŒ€ìƒ(ì˜ˆ: íŒŒì¼, ì½˜ì†”, ë„¤íŠ¸ì›Œí¬ ë“±)ì— ì „ë‹¬í•˜ëŠ” ì—­í• 
        file_handler = logging.FileHandler(log_filename, encoding='utf-8') # FileHandlerë¥¼ ìƒì„±í•´ ë¡œê·¸ë¥¼ íŒŒì¼ì— ê¸°ë¡
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)    # ìœ„ í¬ë§·ë“¤ì„ í•¸ë“¤ëŸ¬ì— ì„¤ì •, ìœ„ í¬ë§·ëŒ€ë¡œ ë¡œê·¸ ë©”ì‹œì§€ê°€ ì¶œë ¥
        file_logger.addHandler(file_handler)    # ìƒì„±í•œ í•¸ë“¤ëŸ¬ë¥¼ file_loggerì— ì¶”ê°€, ì´ í•¸ë“¤ëŸ¬ë¥¼ í†µí•´ ë¡œê·¸ ë©”ì‹œì§€ê°€ 'íŒŒì¼'ì— ê¸°ë¡
    return file_logger

# --- 1. í™˜ê²½ ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜ (ê·¸ë˜í”„ í•œê¸€ ì„¤ì • ë° DB ì ‘ì†) ---
warnings.filterwarnings("ignore")
try:
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False
except Exception:
    print("í°íŠ¸ ì„¤ì • ê²½ê³ : 'Malgun Gothic' í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

DB_HOST = "192.168.0.242"
DB_PORT = 3306
DB_USER = "asi_agent"
DB_PASSWORD = "agent@asi"

# Gemini API í‚¤ 
genai.configure(api_key="AIzaSyDGQWJ6sQWfc8JToxCw9ioXFegBIXdHQLE")
gemini_model = genai.GenerativeModel("gemini-2.5-flash-preview-05-20")

# --- RAG ê´€ë ¨ ì„¤ì • ë° í•¨ìˆ˜ ---
try:
    # 1. SQL ìƒì„±ì„ ìœ„í•œ RAG DB - DBì— ë‚˜ì™€ìˆëŠ” value 
    sql_rag_client = chromadb.PersistentClient(path="chroma_db")    # ChromaDB(ë²¡í„°DB)ì˜ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±
    sql_rag_collection = sql_rag_client.get_collection(name="device_object_names") # í•´ë‹¹ DBì—ì„œ íŠ¹ì • ì»¬ë ‰ì…˜(í…Œì´ë¸”) ê°€ì ¸ì˜´ - RAG_DB_GIST.pyì—ì„œ ìƒì„±í•œ ì»¬ë ‰ì…˜ ì´ë¦„ í™•ì¸ í•„ìš”
    print("âœ… SQL RAG DB ë¡œë”© ì„±ê³µ.")
    
    # 2. ë…¼ë¬¸ ì§€ì‹ íƒìƒ‰ì„ ìœ„í•œ RAG DB 07.01 15:42 ì¶”ê°€
    paper_rag_client = chromadb.PersistentClient(path="chroma_db_paper")
    paper_rag_collection = paper_rag_client.get_collection(name="paper_rag") # pdf_RAG.pyì—ì„œ ìƒì„±í•œ ì»¬ë ‰ì…˜ ì´ë¦„
    print("âœ… Paper RAG DB ë¡œë”© ì„±ê³µ.")

    # [NEW] 3. ì§ˆë¬¸-SQL ì¡±ë³´ë¥¼ ìœ„í•œ RAG DB - 07.03 15:51 ì¶”ê°€
    qa_rag_client = chromadb.PersistentClient(path="chroma_db")
    qa_rag_collection = qa_rag_client.get_collection(name="gist_qa_v1")
    print("âœ… Q&A RAG DB ë¡œë”© ì„±ê³µ.")

except Exception as e:
    print(f"âŒ Vector DB ë¡œë”© ì‹¤íŒ¨: {e}")
    sys.exit()

# ê¸°ì¡´ retrieve_context í•¨ìˆ˜ëŠ” ì´ë¦„ì„ ëª…í™•í•˜ê²Œ ë³€ê²½ - 07.01 15:44 ìˆ˜ì •
def retrieve_sql_context(query: str, n_results: int = 10) -> str:
    """[SQL ìƒì„±ìš©] ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ë†’ì€ ì¥ë¹„/ì„¼ì„œ ì´ë¦„ì„ Vector DBì—ì„œ ê²€ìƒ‰"""
    if sql_rag_collection.count() == 0:
        return "ì°¸ì¡°í•  SQL ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
    
    query_embedding = genai.embed_content(
        model="models/embedding-001",
        content=query,
        task_type="RETRIEVAL_QUERY"
    )['embedding']
    
    results = sql_rag_collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )
    
    retrieved_docs = results['documents'][0]
    context_str = "\n- ".join(retrieved_docs)
    return "- " + context_str if context_str else "ê´€ë ¨ëœ ì¥ë¹„/ì„¼ì„œ ì´ë¦„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# [NEW] ë…¼ë¬¸ RAG ê²€ìƒ‰ì„ ìœ„í•œ ìƒˆë¡œìš´ í•¨ìˆ˜ ì¶”ê°€ - 07.01 15:44 ìˆ˜ì •
# [ìˆ˜ì •] í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì¶œì²˜(ì†ŒìŠ¤) íŒŒì¼ëª…ë„ ë°˜í™˜í•˜ë„ë¡ ë³€ê²½
def retrieve_paper_context(query: str, n_results: int = 5) -> tuple[str, list]:
    """[ì§€ì‹ íƒìƒ‰ìš©] Vector DBì—ì„œ ì»¨í…ìŠ¤íŠ¸ì™€ í•´ë‹¹ ì¶œì²˜ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜"""
    if paper_rag_collection.count() == 0:
        return "ì°¸ì¡°í•  ë…¼ë¬¸ ì§€ì‹ ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", []

    # 1. [ì¶”ê°€] Google ëª¨ë¸ë¡œ ì§ì ‘ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤. (768ì°¨ì› ë²¡í„° ìƒì„±)
    query_embedding = genai.embed_content(
        model="models/embedding-001",
        content=query,
        task_type="RETRIEVAL_QUERY"
    )['embedding']
    
    # 2. [ìˆ˜ì •] í…ìŠ¤íŠ¸ ëŒ€ì‹  ìƒì„±ëœ ë²¡í„°(ì„ë² ë”©)ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
    results = paper_rag_collection.query(
        query_embeddings=[query_embedding], # query_texts -> query_embeddings
        n_results=n_results,
        include=['documents', 'metadatas']  # ê²€ìƒ‰ ê²°ê³¼ì— ë¬¸ì„œë‚´ìš©, ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ ë°˜í™˜
    )
    
    retrieved_chunks = results['documents'][0]
    source_files = list(set(meta['filename'] for meta in results['metadatas'][0] if 'filename' in meta))
    context_str = "\n\n---\n\n".join(retrieved_chunks)
    
    return context_str, source_files

# [NEW] ì§ˆë¬¸-SQL ì¡±ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜ ì¶”ê°€ 07.03 15:53
def retrieve_qa_examples(query: str, n_results: int = 3) -> str:
    """[Q&Aìš©] ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸-SQL ìŒì„ Vector DBì—ì„œ ê²€ìƒ‰"""
    if qa_rag_collection.count() == 0:
        return "ì°¸ê³ í•  Q&A ì˜ˆì‹œê°€ ì—†ìŠµë‹ˆë‹¤."

    query_embedding = genai.embed_content(
        model="models/embedding-001",
        content=query,
        task_type="RETRIEVAL_QUERY"
    )['embedding']
    
    results = qa_rag_collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results
    )
    
    retrieved_docs = results['documents'][0]
    # Q: ...\nSQL: ...\n--- ì‹ìœ¼ë¡œ format!
    qa_examples = []
    for doc in retrieved_docs:
        # md/jsonl êµ¬ì¡°ë¼ë©´ íŒŒì‹±í•´ì„œ Q/SQL ë¶„ë¦¬, ì˜ˆì‹œ í¬ë§·íŒ…
        # ì˜ˆ: {"question": "...", "sql_query": "..."}
        try:
            import json
            item = json.loads(doc)
            qa_examples.append(f"Q: {item['question']}\nSQL: {item['sql_query']}\n---")
        except Exception:
            qa_examples.append(doc) # fallback
    return "\n".join(qa_examples) if qa_examples else "ê´€ë ¨ëœ Q&A ì˜ˆì‹œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    # # ê° ì˜ˆì‹œë¥¼ êµ¬ë¶„ì„ ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë³´ê¸° ì¢‹ê²Œ í•©ì¹¨
    # return "\n\n---\n\n".join(retrieved_docs) if retrieved_docs else "ê´€ë ¨ëœ Q&A ì˜ˆì‹œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# --- <<ìˆ˜ì •>> í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: Planner, SQL, Python í…œí”Œë¦¿ ìµœì‹ í™” ---
planner_prompt_template = """### Instruction:
You are an expert planner AI. Your job is to understand the user's request and create a step-by-step plan to fulfill it.

# [CRITICAL RULE] When the user asks for a calculated or aggregated value (e.g., average, sum, count, min, max),
# the `db_querier` tool is responsible for performing that calculation directly within the SQL query.
# Your plan's description for the `db_querier` step MUST reflect this calculation task.
# Do NOT create a separate synthesizer/summarizer step for simple calculations that the database can handle.

# [CRITICAL RULE for RAG-based Planning]
# If the RAG context provides a complete, single SQL query that solves the entire user request, your plan MUST consist of a single 'db_querier' step. Do NOT break it down into multiple steps.

# [CRITICAL CONTEXTUAL PLANNING RULE]
# When the user's question contains pronouns (e.g., 'ê·¸ê²ƒ', 'ê·¸ê²Œ', that, it) or contextual references ('ë°©ê¸ˆ', 'ì•„ê¹Œ', 'ì´ì „', 'just now'), you MUST examine the `Conversation History` to resolve the context.
# 1. If the answer is DIRECTLY stated in the previous agent response, your plan MUST use the `memory_retriever` tool.
# 2. If the history only provides CONTEXT for a new database query (e.g., identifying "that PDU" is "PDU 16"), your plan MUST create a more SPECIFIC `db_querier` step with a description that includes the resolved context.

# [CRITICAL RULE for Final Step]
# - For requests that ask for a "report", "document", or "summary file" (ë³´ê³ ì„œ, ë¬¸ì„œ, ìš”ì•½ íŒŒì¼), your plan MUST end with the `report_generator` tool.
# - For all other questions that require a synthesized answer from data, the plan should end with `answer_synthesizer`.
# - A single plan MUST NOT contain both `report_generator` and `answer_synthesizer`. Choose only one as the final step.

# [CRITICAL RULE for Comparisons]
# For questions that require comparing two or more items (e.g., "compare A and B"),
# you MUST create a single `db_querier` step that fetches all the necessary information for the comparison at once.
# Do NOT create separate `db_querier` steps for each item.

# Bad Plan:
# - Step 1: db_querier (Get data for A)
# - Step 2: db_querier (Get data for B)
# - Step 3: synthesizer (Compare A and B)

# Good Plan:
# - Step 1: db_querier (Get data for both A and B in a single query)
# - Step 2: synthesizer (Analyze the combined data and compare)

You have the following tools available:
- `db_querier`: Used to query a database to retrieve data.
- `visualizer`: Used to create a plot or chart from data.
- `anomaly_detector`: Used to analyze data to find anomalies (e.g., abnormally high values).
- `general_qa`: Used for conceptual or explanatory questions about technologies, principles, or definitions (e.g., "What is PUE?", "Explain how a bus duct works."). This tool will search a knowledge base of technical documents to answer the question.
- `answer_synthesizer`: Uses retrieved data to formulate a final, natural language answer to the user's original question.
- `memory_retriever`: When the user asks a follow-up question (using pronouns like 'that', 'it', 'ê·¸ê²ƒ', 'ì´ì „','ì•„ê¹Œ','ë°©ê¸ˆ','ê·¸ê²Œ') that can be answered from the recent conversation history, use this tool to extract the answer directly from memory without accessing the database.
- data_summarizer: Summarizes a large table of data into key statistics before final analysis.
- `report_generator`: Used to combine data, tables, and plots from previous steps into a single, structured report in Markdown format. Use this when the user asks for a "report", "summary document", or "briefing".


Based on the user's request, create a plan as a JSON array of steps. Each step must have a "tool" and a "description".

### Examples

**Request:** "GIST PUE ì•Œë ¤ì¤˜"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "Query the database to get the latest PUE value."
    }}
]

**Request (with simulated RAG context):**
- *User's Request:* "6ì›” í•œë‹¬ê°„ ì˜¨ë„ ê²½ë³´ê°€ ì£¼ 10íšŒ ì´ìƒ ë°œìƒí•œ í•­ì˜¨í•­ìŠµê¸°ë¥¼ ì°¾ì•„ì„œ, í•´ë‹¹ ê¸°ê°„ ì¤‘ ìµœê³  ì˜¨ë„ë¥¼ ì•Œë ¤ì¤˜."
- *Simulated RAG Context:* (A complete, single SQL query for this question has been found in the RAG examples)

**Plan (GOOD - follows the RAG example):**
```json
[
    {{
        "tool": "db_querier",
        "description": "RAG ì˜ˆì‹œì—ì„œ ì°¾ì€ ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬, 6ì›” í•œ ë‹¬ê°„ ì˜¨ë„ ê²½ë³´ê°€ ì£¼ 10íšŒ ì´ìƒ ë°œìƒí•œ í•­ì˜¨í•­ìŠµê¸°ì˜ ìµœê³  ì˜¨ë„ì™€ ì‹œê°„ì„ í•œ ë²ˆì— ì¡°íšŒí•©ë‹ˆë‹¤."
    }}
]

**Request:** "ì´ë²ˆ ì£¼ ë™ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ì´ ì–¼ë§ˆì•¼? ê·¸ë˜í”„ë¡œë„ ë³´ì—¬ì¤˜"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ì´ë²ˆ ì£¼ ë™ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "visualizer",
        "description": "1ë‹¨ê³„ì—ì„œ ì–»ì€ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ì„ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ìƒì„±í•©ë‹ˆë‹¤."
    }}
]

**Request:** "PDU Rack ì¤‘ ì „ë ¥ì— ì´ìƒì´ ìˆëŠ” ê²ƒì„ ì•Œë ¤ì¤˜"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ë¶„ì„ì„ ìœ„í•´ ìµœê·¼ 3ì¼ê°„ì˜ ëª¨ë“  PDU ë™ë³„ ì „ë ¥ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "anomaly_detector",
        "description": "1ë‹¨ê³„ì—ì„œ ì–»ì€ ì „ì²´ ì „ë ¥ ë°ì´í„°ì—ì„œ í†µê³„ì ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ë³´ì´ëŠ” PDU ë™ì„ íƒì§€í•©ë‹ˆë‹¤."
    }}
]   

**Request:** "ì˜¨ìŠµë„ê³„ 17ë²ˆ ì˜¨ë„ê°€ 25ë„ë¥¼ ë„˜ì—ˆì–´?"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ì˜¨ìŠµë„ê³„ 17ë²ˆì˜ í˜„ì¬ ì˜¨ë„ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "answer_synthesizer",
        "description": "1ë‹¨ê³„ì—ì„œ ì¡°íšŒí•œ ì˜¨ë„ ê°’ê³¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸('25ë„ë¥¼ ë„˜ì—ˆì–´?')ì„ ë¹„êµí•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ 'ì˜ˆ/ì•„ë‹ˆì˜¤' ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }}
]

**Request:** (After being told that Chamber 5 had the lowest temperature) "ê·¸ê²Œ ëª‡ ë²ˆ ì±”ë²„ì•¼?"
**Plan:**
```json
[
    {{
        "tool": "memory_retriever",
        "description": "Extract the chamber number from the previous agent's response in the conversation history."
    }}
]

**Request:** "ìµœê·¼ 6ì‹œê°„ë™ì•ˆ PDUì˜ ì „ë ¥ì´ ì–¼ë§ˆë‚˜ ë³€ë™ëì–´?"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ìµœê·¼ 6ì‹œê°„ ë™ì•ˆì˜ ëª¨ë“  PDU ì „ë ¥ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "data_summarizer",
        "description": "1ë‹¨ê³„ì—ì„œ ì–»ì€ ë°©ëŒ€í•œ ì „ë ¥ ë°ì´í„°ë¥¼ ìµœì†Œ/ìµœëŒ€/í‰ê·  ê°’ ë“± í•µì‹¬ í†µê³„ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "answer_synthesizer",
        "description": "2ë‹¨ê³„ì—ì„œ ìš”ì•½ëœ í†µê³„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë ¥ ë³€ë™ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }}
]

**Request (with conversation history):**
- *Conversation History:*  #ëŠ” ë²ˆí˜¸
  - user: í˜„ì¬ PDU #ë²ˆì— ì¸¡ì •ë˜ëŠ” ì§€í‘œë“¤ì„ ì•Œë ¤ì¤˜
  - agent: PDU #ë²ˆì— ì¸¡ì •ë˜ëŠ” ì§€í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: PDU_#_Rack_%-output_current: 11.9 ...
- *Current User Request:* "ë°©ê¸ˆ PDUì˜ ì „ë¥˜ëŠ” ëª‡ì´ì•¼?"

**Plan (GOOD - uses history to make a specific plan):**
```json
[
    {{
        "tool": "db_querier",
        "description": "ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìê°€ ì§ˆë¬¸í•˜ëŠ” 'ë°©ê¸ˆ PDU'ê°€ 'PDU #'ì„ì„ íŒŒì•…í•˜ê³ , PDU #ì˜ í˜„ì¬ ì „ë¥˜ ê°’ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }}
]
**Plan (BAD - ignores history and makes a generic plan):**
```json
[
    {{
        "tool": "db_querier",
        "description": "ê°€ì¥ ìµœê·¼ì˜ PDU ì „ë¥˜ ê°’ì„ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }}
]

**Request:** "ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆì˜ PDUë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ìœ¼ë¡œ ì£¼ê°„ ë³´ê³ ì„œë¥¼ ë§Œë“¤ì–´ì£¼ê³ , ìƒìœ„ 5ê°œ PDUë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ë³´ì—¬ì¤˜."
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ëª¨ë“  PDUì˜ ì „ë ¥ ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "visualizer",
        "description": "ì¡°íšŒëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ PDUë³„ í‰ê·  ì „ë ¥ ì‚¬ìš©ëŸ‰ ìƒìœ„ 5ê°œë¥¼ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ìƒì„±í•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "report_generator",
        "description": "1, 2ë‹¨ê³„ì˜ ë°ì´í„°ì™€ ê·¸ë˜í”„ë¥¼ ì¢…í•©í•˜ì—¬ ì£¼ê°„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
    }}
]

**Request:** "ì–´ì œ í•«ìŠ¤íŒŸì´ ë°œìƒí–ˆì–´?"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ì–´ì œ ABë™ì—ì„œ ë°œìƒí•œ í•«ìŠ¤íŒŸ(í‰ê·  ì˜¨ë„ 35Â°C ì´ˆê³¼) ë°ì´í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "data_summarizer",
        "description": "1ë‹¨ê³„ì—ì„œ ì¡°íšŒëœ í•«ìŠ¤íŒŸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë°œìƒ ê±´ìˆ˜, ìµœê³  ì˜¨ë„, ë°œìƒ ì‹œê°„ ë“± í•µì‹¬ ì •ë³´ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "answer_synthesizer",
        "description": "2ë‹¨ê³„ì—ì„œ ìš”ì•½ëœ í•«ìŠ¤íŒŸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì–´ì œ í•«ìŠ¤íŒŸ ë°œìƒ ì—¬ë¶€ì™€ ìƒì„¸ ë‚´ì—­ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }}
]

**Request:** "ë°ì´í„°ì„¼í„°ì— íŠ¹ì´ì‚¬í•­ì´ë‚˜ ë¬¸ì œì  ìˆì–´?"
**Plan:**
```json
[
    {{
        "tool": "db_querier",
        "description": "ì–´ì œë¶€í„° í˜„ì¬ê¹Œì§€ ë°œìƒí•œ í•«ìŠ¤íŒŸ(ABë™ 35Â°C ì´ˆê³¼)ì´ ìˆì—ˆëŠ”ì§€ ì¡°íšŒí•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "db_querier",
        "description": "ê°™ì€ ê¸°ê°„ ë™ì•ˆì˜ ì£¼ìš” ì§€í‘œ(ì˜¨ë„, ìŠµë„, PUE)ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ í†µê³„ì  ì´ìƒì¹˜ë¥¼ íƒì§€í•  ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "anomaly_detector",
        "description": "2ë‹¨ê³„ì—ì„œ ì¡°íšŒëœ ì£¼ìš” ì§€í‘œ ë°ì´í„°ì—ì„œ ë¹„ì •ìƒì ì¸ íŒ¨í„´ì´ë‚˜ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤."
    }},
    {{
        "tool": "answer_synthesizer",
        "description": "1, 3ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬, í•«ìŠ¤íŒŸ ë°œìƒ ì—¬ë¶€ì™€ í†µê³„ì  ì´ìƒì¹˜ ë¶„ì„ì„ í¬í•¨í•œ ë°ì´í„°ì„¼í„°ì˜ ì „ì²´ì ì¸ íŠ¹ì´ì‚¬í•­ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."
    }}
]




**User Request:** "{user_question}"
**Plan:**
```json
"""

# SQL í”„ë¡¬í”„íŠ¸ ì¿¼ë¦¬
sql_gen_prompt_template = """### Instruction: Your Task and Decision-Making Process

You are an expert MySQL engineer. Your goal is to generate one single, perfect, and efficient query to answer the user's question. Follow this step-by-step process meticulously:
ë‚ ì§œì— ê´€í•œ ì§ˆë¬¸ ì¤‘ì— ì—°ë„ê°€ ì—†ìœ¼ë©´ ì˜¬í•´(2025ë…„) ì—°ë„ë¡œ ê²€ìƒ‰í•˜ì„¸ìš”.

**Step 1: Check for a "Cheat Sheet" Match in RAG Examples.**
- First, look at the "Retrieved Context from Q&A Examples". Is the `User's Current Question` almost **identical** to one of the examples?
- If YES, your highest priority is to copy and adapt the corresponding "ëª¨ë²” SQL ì¿¼ë¦¬". This is your most critical rule. This step takes precedence over all others.

**Step 2: If No Perfect Match, Synthesize a New Query.**
- If there is no identical example, you must build a new query from scratch. To do this, you MUST use the following resources in this order of importance:
  1.  **The User's Current Question**: This is the primary goal you must fulfill.
  2.  **Retrieved Q&A Examples**: Use these as **inspiration** for the query structure and logic (e.g., how to compare items, how to find min/max, how to use window functions).
  3.  **Retrieved Knowledge Base (Terms)**: Use these to find the exact `description` or `objectName` for the devices mentioned in the user's question.
  4.  **General Querying Rules (Best Practices)**: You must follow these rules for efficiency and correctness.
  5.  **MySQL Compatibility Rules**: Finally, ensure your query is syntactically correct according to these rules.

**Step 3: A Note on Conversation History.**
- The `Conversation History` is mainly for understanding pronouns or very direct follow-ups. For generating new SQL, the RAG examples and the current question are far more important.

---
### General Querying Rules (For reference in Step 2)
1.  **Specific vs. General**: Use `o.description` for specific devices, `o.objectName` for general metrics like 'PUE'.
2.  **Data Validation**: ALWAYS use `v.value BETWEEN 0 AND 100` for temperature and humidity. Always use `v.value >= 0` for power summations.
3.  **Completeness**: For min/max/avg requests, select the device name (`description`) along with the value.
4.  **Averages for General Queries**: If the user asks for a trend of a device type without a specific number, calculate the overall average by removing the device name from `SELECT` and `GROUP BY`.
5.  **Efficiency**: Use `GROUP BY` for trends over long periods. Do not use `UNION ALL` for trend-vs-average plots; let Python handle the final average calculation.
6.  **Clarity**: Use parentheses `()` when mixing `AND` and `OR` to define the order of operations.

---

### MySQL Compatibility Rules:
1. Database uses `sql_mode=only_full_group_by`, so ALL columns in SELECT must be in GROUP BY or used in aggregate functions.
2. DO NOT use WITH ROLLUP syntax.
3. Avoid complex COALESCE expressions with non-aggregated columns.
4. If you need overall averages and detailed data together, use UNION or separate queries.
5. For date grouping, use `GROUP BY DATE(timestamp)` and include `DATE(timestamp)` in the SELECT list.

### Conversation History:
{conversation_history}

### Retrieved Context from Knowledge Base (Terms):
# DBì˜ ì‹¤ì œ ìš©ì–´ë“¤ì…ë‹ˆë‹¤. 
{retrieved_context}

### Retrieved Context from Q&A Examples:
# í˜„ì¬ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ê³¼ê±°ì˜ ì§ˆë¬¸ê³¼ ëª¨ë²” SQL ì¿¼ë¦¬ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¨¼ì € ì—¬ê¸°ì„œ ê²€ìƒ‰í•˜ê³  íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
{retrieved_qa_examples}

### Korean-to-Database Term Mapping:
- 'í•­ì˜¨í•­ìŠµê¸°': `Constant_Temp_and_Humi_Chamber_`
  - 'í•­ì˜¨í•­ìŠµê¸° ì˜¨ë„': `Constant_Temp_and_Humi_Chamber_#-current_temperature` (# = ë²ˆí˜¸)
  - 'í•­ì˜¨í•­ìŠµê¸° ìŠµë„': `Constant_Temp_and_Humi_Chamber_#-current_humidity` (# = ë²ˆí˜¸)
  - 'í•­ì˜¨í•­ìŠµê¸° ìš´ì „ ìƒíƒœ': `Constant_Temp_and_Humi_Chamber_#-set_running_status` (# = ë²ˆí˜¸)
- 'ì˜¨ìŠµë„ê³„ #ë²ˆ ì˜¨ë„': 'Thermo_Hygrometer_#-temperature_ch2'  # ch2(í™˜ê¸° ì˜¨ë„)ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •(# = ë²ˆí˜¸)
- 'ì˜¨ìŠµë„ê³„ #ë²ˆ ìŠµë„': 'Thermo_Hygrometer_#-humidity_ch2'   # ìŠµë„ë„ ch2ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •(# = ë²ˆí˜¸)
- 'ë¶„ì „ë°˜': `Distribution_Board_`
- 'ë²„ìŠ¤ë•íŠ¸': `Bus_Duct_`
- 'ì „ë ¥', 'íŒŒì›Œ': `power` or `output_power`
- 'ì˜¨ë„': `temperature` or `current_temperature`
- 'ìŠµë„': `humidity` or `current_humidity`
- 'PDU': `PDU_`
- 'PUE': `description = 'Post_processing_data-PUE'`
- 'í•«ìŠ¤íŒŸ': `AB_hot_average_temperture`

# ë¶„ì „ë°˜ ëª…ì¹­ ë§¤í•‘ (Distribution Board Naming Map):
# 1. ë¶„ì „ë°˜ 1ë²ˆ (LP1 íŒ¨ë„)
- ì‚¬ìš©ì ìš©ì–´: "ë¶„ì „ë°˜ 1", "1ë²ˆ LP", "1ë²ˆ LP1 íŒ¨ë„"
- DB íŒ¨í„´: `Distribution_Board_1_LP_1_Panel-[ì¸¡ì •í•­ëª©]`
- ì˜ˆì‹œ: "ë¶„ì „ë°˜ 1ë²ˆ LP1 íŒ¨ë„ì˜ ì£¼íŒŒìˆ˜" -> `description = 'Distribution_Board_1_LP_1_Panel-frequency'`

# 2. ë¶„ì „ë°˜ 2ë²ˆ (UPS íŒ¨ë„)
- ì‚¬ìš©ì ìš©ì–´: "ë¶„ì „ë°˜ 2", "2ë²ˆ UPS", "UPS íŒ¨ë„"
- DB íŒ¨í„´: `Distribution_Board_2_UPS_Panel-[ì¸¡ì •í•­ëª©]`
- ì˜ˆì‹œ: "ë¶„ì „ë°˜ 2ë²ˆ UPS íŒ¨ë„ì˜ ì „ë ¥" -> `description = 'Distribution_Board_2_UPS_Panel-power'`

# 3. ë¶„ì „ë°˜ 3ë²ˆ (LP-AC1 íŒ¨ë„)
- ì‚¬ìš©ì ìš©ì–´: "ë¶„ì „ë°˜ 3", "3ë²ˆ AC1", "3ë²ˆ LP-AC1 íŒ¨ë„"
- DB íŒ¨í„´: `Distribution_Board_3_LP_AC1_Panel-[ì¸¡ì •í•­ëª©]`
- ì˜ˆì‹œ: "ë¶„ì „ë°˜ 3ë²ˆ LP-AC1 íŒ¨ë„ì˜ Rìƒ ë¼ì¸ ì „ì••" -> `description = 'Distribution_Board_3_LP_AC1_Panel-line_voltage_r'`

# 4. ë¶„ì „ë°˜ 4ë²ˆ (LP-AC2 íŒ¨ë„)
- ì‚¬ìš©ì ìš©ì–´: "ë¶„ì „ë°˜ 4", "4ë²ˆ AC2", "4ë²ˆ LP-AC2 íŒ¨ë„"
- DB íŒ¨í„´: `Distribution_Board_4_LP_AC2_Panel-[ì¸¡ì •í•­ëª©]`
- ì˜ˆì‹œ: "ë¶„ì „ë°˜ 4ë²ˆ LP-AC2 íŒ¨ë„ì˜ ì—­ë¥ " -> `description = 'Distribution_Board_4_LP_AC2_Panel-power_factor'`

# 5. ë¶„ì „ë°˜ 5ë²ˆ (ë©”ì¸ íŒ¨ë„)
- ì‚¬ìš©ì ìš©ì–´: "ë¶„ì „ë°˜ 5", "5ë²ˆ ë©”ì¸"
- DB íŒ¨í„´: `Distribution_Board_5_Main-[ì¸¡ì •í•­ëª©]`
- ì˜ˆì‹œ: "ë¶„ì „ë°˜ 5ë²ˆ ë©”ì¸ Sìƒ ì „ë¥˜" -> `description = 'Distribution_Board_5_Main-current_s'`

# ì¸¡ì •í•­ëª© ì¼ë°˜ ê·œì¹™
- 'ì „ë¥˜': ë³„ë„ ìƒ(r, s, t) ì–¸ê¸‰ ì—†ìœ¼ë©´ `-current_r`
- 'ë¼ì¸ ì „ì••': ë³„ë„ ìƒ ì–¸ê¸‰ ì—†ìœ¼ë©´ `-line_voltage_r`
- 'ìƒ ì „ì••': ë³„ë„ ìƒ ì–¸ê¸‰ ì—†ìœ¼ë©´ `-phase_voltage_r`
- 'ì „ë ¥': `-power`
- 'ì—­ë¥ ': `-power_factor`
- 'ì£¼íŒŒìˆ˜': `-frequency`

### í•­ì˜¨í•­ìŠµê¸° ìƒíƒœ ë§¤í•‘ (Thermo-Hygrostat Status Mapping):
# ì‚¬ìš©ìê°€ í•­ì˜¨í•­ìŠµê¸°ì˜ íŠ¹ì • ì‘ë™ ìƒíƒœì— ëŒ€í•´ ì§ˆë¬¸í•  ê²½ìš°, ì•„ë˜ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ 'description'ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
# ì°¸ê³ : ì•„ë˜ í•­ëª©ë“¤ì˜ valueê°€ 1ì´ë©´ 'ì‘ë™ ì¤‘(ON)', 0ì´ë©´ 'ì •ì§€(OFF)'ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

# 1. ëƒ‰ë°© (Cooling)
- ì‚¬ìš©ì ìš©ì–´: "ëƒ‰ë°©", "ëƒ‰ë°© ì¤‘", "ëƒ‰ë°© ëª¨ë“œ"
- DB íŒ¨í„´: `Constant_Temp_and_Humi_Chamber_[ì¥ë¹„ë²ˆí˜¸]-running_status_coldroom`

# 2. ë‚œë°© (Heating)
- ì‚¬ìš©ì ìš©ì–´: "ë‚œë°©", "ë‚œë°© ì¤‘", "ë‚œë°© ëª¨ë“œ"
- DB íŒ¨í„´: `Constant_Temp_and_Humi_Chamber_[ì¥ë¹„ë²ˆí˜¸]-running_status_warmroom`

# 3. ì œìŠµ (Dehumidifying)
- ì‚¬ìš©ì ìš©ì–´: "ì œìŠµ", "ì œìŠµ ì¤‘", "ì œìŠµ ì‘ë™"
- DB íŒ¨í„´: `Constant_Temp_and_Humi_Chamber_[ì¥ë¹„ë²ˆí˜¸]-running_decrease_humidity`

# 4. ê°€ìŠµ (Humidifying)
- ì‚¬ìš©ì ìš©ì–´: "ê°€ìŠµ", "ê°€ìŠµ ì¤‘", "ê°€ìŠµ ê¸°ëŠ¥"
- DB íŒ¨í„´: `Constant_Temp_and_Humi_Chamber_[ì¥ë¹„ë²ˆí˜¸]-running_increase_humidity`

### ì‹œë©˜í‹± ê·¸ë£¹ ì •ì˜ (Semantic Group Definitions):
# ì‚¬ìš©ìê°€ ì¶”ìƒì ì¸ ê·¸ë£¹(ì˜ˆ: ì „ì²´ ì „ë ¥, IT ë¶€í•˜)ì— ëŒ€í•´ ì§ˆë¬¸í•  ê²½ìš°, ì•„ë˜ ì •ì˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

# 1. 'ë°ì´í„°ì„¼í„° ì´ ì „ë ¥ ì†Œë¹„ëŸ‰ (Total Power Consumption)'
#    - 'ì´ ì „ë ¥ëŸ‰'ì€ ì•„ë˜ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ì¥ë¹„ì˜ 'ìµœì‹ ' ì „ë ¥ ê°’ì„ í•©ì‚°í•œ ê²ƒì…ë‹ˆë‹¤.
#    - ë‹¨ìœ„ëŠ” ì™€íŠ¸(W)ì´ë¯€ë¡œ, í‚¬ë¡œì™€íŠ¸(kW)ë¡œ ë³€í™˜í•˜ë ¤ë©´ 1000ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì•¼ í•©ë‹ˆë‹¤.
#    - í¬í•¨ë˜ì–´ì•¼ í•  í•­ëª©ë“¤ (LIKE ì‚¬ìš©):
#      - 'PDU_%-output_power'
#      - 'Distribution_Board_%-power'
#      - 'Bus_Duct_%-power'
#      - 'Chamber_Power_Meter_%-active_power' -- 'active_power'ê°€ ì‹¤ì œ ìœ íš¨ ì „ë ¥ì…ë‹ˆë‹¤.
#      - 'Post_processing_data-Server_power'
#    - ì œì™¸ë˜ì–´ì•¼ í•  í•­ëª©ë“¤:
#      - 'power_factor'(ì—­ë¥ ), 'reactive_power'(ë¬´íš¨ì „ë ¥) ë“±ì€ ì‹¤ì œ ì†Œë¹„ ì „ë ¥ì´ ì•„ë‹ˆë¯€ë¡œ í•©ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.

### Additive Querying Rules:
1. **Choose the correct column (`objectName` vs. `description`)**:
   - For a **specific device and metric** (e.g., "PDU 1's power"), query the `o.description` column for the highest accuracy (e.g., `o.description LIKE 'PDU_1_%-output_power'`).
   - For a **general metric name** that acts as a standalone point (e.g., "PUE"), query the `o.objectName` column (e.g., `o.objectName = 'PUE'`).
2. **Use Precise `LIKE` Patterns**:
   - For 'PDU 1': use `LIKE 'PDU_1_%'`. The underscore `_` is a wildcard for a single character.
   - For 'í•­ì˜¨í•­ìŠµê¸° 2ë²ˆ ì˜¨ë„': use `LIKE 'Constant_Temp_and_Humi_Chamber_2-current_temperature'`
   - For 'í•­ì˜¨í•­ìŠµê¸° 2ë²ˆ ìŠµë„': use `LIKE 'Constant_Temp_and_Humi_Chamber_2-current_humidity'`
3. **Correctly `JOIN` tables** (`fms_object_value` as v, `fms_object_list` as o, `fms_device_list` as d) when you need information across them.
4. **Distinguish 'current' vs. 'overall' requests**: For "current" or "latest" data, use `ORDER BY v.timestamp DESC LIMIT 1`. For overall trends or averages without a time constraint, do not limit the time range.
5.  **Apply Data Validation Filters**: To prevent outliers, add reasonable range conditions to the `WHERE` clause.
    - For temperature (`ì˜¨ë„`) queries, ALWAYS add `AND v.value BETWEEN 0 AND 100`.
    - For humidity (`ìŠµë„`) queries, ALWAYS add `AND v.value BETWEEN 0 AND 100`.
6.  **Assume the current year** if a date is mentioned without a year. The current date is **{current_date}**.
7.  **Single Query**: Generate only one single, executable MySQL query.
8.  **Provide Complete Information**: When the user asks for a superlative (min, max, avg), you MUST select both the value itself AND the `description` or `deviceName` of the record it belongs to. Do not select only the value.
9. **Overall Average for General Queries**: If the user asks for a trend of a device type (e.g., 'PDU', 'ì˜¨ìŠµë„ê³„') *without* specifying a number or a specific name, generate a query that calculates the **overall average** for all devices of that type. Do this by removing the specific device name column (like `o.description`) from the `SELECT` and `GROUP BY` clauses.
10.  **Aggregate for Trends**: For trend analysis over long periods (e.g., 'a week', 'a month'), DO NOT fetch raw data. Instead, calculate hourly or daily averages directly in the SQL query using `AVG()` and `GROUP BY`. This is much more efficient.
12. **Avoid Complex Unions for Trend + Average**: For a "trend vs. average" plot, DO NOT use `UNION ALL` to combine trend data and the overall average in one query. It is much more stable and efficient to:
    1. Generate a simple query for the trend data only (e.g., `GROUP BY DATE(timestamp)`).
    2. Let the subsequent Python code calculate the overall average from that trend data (e.g., `df['value'].mean()`). This is the preferred method.

---
### Query Examples

#### Example 1: Specific device and metric -> Use `description`
Question: "GISTì˜ PDU 1ë²ˆ ì¥ë¹„ì˜ ì „ë ¥ì„ ì•Œë ¤ì¤˜"
MySQL Query:
USE gist_agent_test;
SELECT v.timestamp, v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description LIKE 'PDU_1_%-output_power' ORDER BY v.timestamp DESC LIMIT 1;

#### Example 2: General standalone metric -> Use `objectName`
Question: "ìµœê·¼ 1ì‹œê°„ ë™ì•ˆì˜ PUE íŠ¸ë Œë“œë¥¼ ë³´ì—¬ì¤˜"
MySQL Query:
USE gist_agent_test;
SELECT v.timestamp, v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.objectName = 'PUE' AND v.timestamp >= NOW() - INTERVAL 1 HOUR ORDER BY v.timestamp ASC;

#### Example 3: Korean term for specific device -> Use `description` with correct pattern
Question: "ì˜¨ìŠµë„ê³„ 2ë²ˆì˜ í˜„ì¬ ì˜¨ë„ëŠ” ì–¼ë§ˆì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT o.description, v.value, v.timestamp FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description LIKE 'Thermo_Hygrometer_2-temperature_ch2' ORDER BY v.timestamp DESC LIMIT 1;

#### Example 4: Daily averages with overall average
Question: "ì§€ë‚œ 7ì¼ê°„ PUE íŠ¸ë Œë“œë¥¼ 1ì¼ ê°„ê²©ìœ¼ë¡œ ë³´ì—¬ì£¼ê³ , í‰ê· ê³¼ ë¹„êµí•´ì¤˜"
MySQL Query:
USE gist_agent_test;
-- ì¼ë³„ PUE í‰ê· 
SELECT DATE(v.timestamp) AS trend_date, AVG(v.value) AS daily_pue FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.objectName = 'PUE' AND v.timestamp >= DATE_SUB(CURDATE(), INTERVAL 7 DAY) GROUP BY DATE(v.timestamp) ORDER BY trend_date ASC;

#### Example 5: í•­ì˜¨í•­ìŠµê¸° ì˜¨ë„/ìŠµë„ ì¡°íšŒ (correct pattern)
Question: "ì–´ì œë¶€í„° ì˜¤ëŠ˜ê¹Œì§€ í•­ì˜¨ í•­ìŠµê¸°ì—ì„œ ê°€ì¥ ë†’ì€ ì˜¨ë„ë¥¼ ê¸°ë¡í•œê²Œ ëª‡ ë²ˆ ì±”ë²„ì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT o.description AS chamber_name, v.value AS highest_temperature_celsius, v.timestamp FROM fms_object_value AS v JOIN fms_object_list AS o ON v.object_ID = o.id WHERE o.description LIKE 'Constant_Temp_and_Humi_Chamber_%-current_temperature' AND v.timestamp >= CURDATE() - INTERVAL 1 DAY AND v.timestamp <= NOW() AND v.value BETWEEN 0 AND 100 ORDER BY v.value DESC LIMIT 1;

#### Example 6: í•­ì˜¨í•­ìŠµê¸° ìŠµë„ ì¡°íšŒ (correct pattern)
Question: "2ë²ˆ í•­ì˜¨í•­ìŠµê¸°ì˜ ìŠµë„ê°€ ì–¼ë§ˆì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT o.description, v.value AS humidity_percent, v.timestamp FROM fms_object_value AS v JOIN fms_object_list AS o ON v.object_ID = o.id WHERE o.description = 'Constant_Temp_and_Humi_Chamber_2-current_humidity' ORDER BY v.timestamp DESC LIMIT 1;

#### Example 7: Finding Min/Max values with device info (BEST PRACTICE)
Question: "ì–´ì œ í•­ì˜¨í•­ìŠµê¸°ì˜ ìµœì € ì˜¨ë„ëŠ” ëª‡ ë„ ì˜€ì–´?"
MySQL Query:
USE gist_agent_test;
SELECT o.description, v.value FROM fms_object_value AS v JOIN fms_object_list AS o ON v.object_ID = o.id WHERE o.description LIKE 'Constant_Temp_and_Humi_Chamber_%-current_temperature' AND DATE(v.timestamp) = CURDATE() - INTERVAL 1 DAY AND v.value BETWEEN 0 AND 100 ORDER BY v.value ASC LIMIT 1;

#### Example 8:  ê²½ë³´ ê´€ë ¨ -- v.value = 1ì¸ 'ê²½ë³´ê°€ í™œì„± ìƒíƒœ'ì„ì„ ì˜ë¯¸
Question: "ì§€ê¸ˆ ê²½ë³´ ëœ¬ í•­ì˜¨í•­ìŠµê¸° ìˆì–´?"
MySQL Query:
USE gist_agent_test;
SELECT o.deviceName, o.objectName, v.timestamp FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.deviceName LIKE 'Constant_Temp_and_Humi_Chamber_%' AND o.objectName LIKE 'warn_%' AND v.value = 1 AND v.timestamp >= NOW() - INTERVAL 2 HOUR ORDER BY v.timestamp DESC;

#### Example 9 : ëƒ‰ë°©, ë‚œë°© ê´€ë ¨ ('ë‚œë°©' ì´ë©´ 'Constant_Temp_and_Humi_Chamber_#-running_status_warmroom', #ëŠ” ë²ˆí˜¸)
Question : "í•­ì˜¨í•­ìŠµê¸° 4ë²ˆ ì§€ê¸ˆ ëƒ‰ë°© ì¤‘ì´ì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT o.description, v.value, v.timestamp FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'Constant_Temp_and_Humi_Chamber_4-running_status_coldroom' ORDER BY v.timestamp DESC LIMIT 1;

#### Example 10 : 'ì´ë²ˆ ì£¼' ë¼ê³  ë¬¼ì—ˆì„ ë•Œ ê¸°ê°„ ì„¤ì •
Question : "ì´ë²ˆ ì£¼ í•­ì˜¨í•­ìŠµê¸° 9ì˜ í‰ê·  ìŠµë„ëŠ” ì–´ëŠì •ë„ì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT o.description, AVG(v.value) AS average_humidity_percent FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'Constant_Temp_and_Humi_Chamber_9-current_humidity' AND WEEK(v.timestamp) = WEEK(CURDATE()) AND YEAR(v.timestamp) = YEAR(CURDATE()) AND v.value BETWEEN 0 AND 100;

#### Example 11 : 
Question : "ì—¬ëŸ¬ ë¶„ì „ë°˜ ì¤‘ í˜„ì¬ ì „ë¥˜ê°€ ê°€ì¥ ë‚®ì€ íŒ¨ë„ì€ ì–´ë””ì´ê³ , ì „ë¥˜ ê°’ì€ ì–¼ë§ˆì•¼? "
MySQL Query:
USE gist_agent_test; 
SELECT o.deviceName, v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.objectName = 'current_r' AND o.deviceName LIKE 'Distribution_Board_%' ORDER BY v.value ASC LIMIT 1;

#### Example 12: ë‘ ê°œë¥¼ ë¹„êµ í•  ë•Œ
Question : "ë¶„ì „ë°˜ 2 UPS íŒ¨ë„ì´ë‘ ë¶„ì „ë°˜ 4 AC2 íŒ¨ë„ ì¤‘ ì–´ë”” ì „ë¥˜ê°€ ë” ë†’ì•„?"
MySQL Query:
USE gist_agent_test; 
SELECT (SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'Distribution_Board_2_UPS_Panel-current_r' ORDER BY v.timestamp DESC LIMIT 1) AS db2_current, (SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'Distribution_Board_4_LP_AC2_Panel-current_r' ORDER BY v.timestamp DESC LIMIT 1) AS db4_current;

#### Example 13: ë‘ ì¥ë¹„ì˜ ìµœì‹  ê°’ ë¹„êµ (íš¨ìœ¨ì ì¸ ë°©ë²•)
Question: "í•­ì˜¨í•­ìŠµê¸° 4ë²ˆê³¼ 5ë²ˆì˜ í˜„ì¬ ì˜¨ë„ë¥¼ ë¹„êµí•´ì¤˜."
MySQL Query:
# This query efficiently finds the latest timestamp for each specified device first, then joins to get the values.
USE gist_agent_test;
SELECT o.description, v.value FROM fms_object_value AS v JOIN (SELECT object_ID, MAX(timestamp) AS max_timestamp FROM fms_object_value WHERE object_ID IN (SELECT id FROM fms_object_list WHERE description IN ('Constant_Temp_and_Humi_Chamber_4-current_temperature', 'Constant_Temp_and_Humi_Chamber_5-current_temperature')) GROUP BY object_ID) AS latest_data ON v.object_ID = latest_data.object_ID AND v.timestamp = latest_data.max_timestamp JOIN fms_object_list AS o ON v.object_ID = o.id;

#### Example 14: ë‘ ì¥ë¹„ì˜ ìµœì‹  ê°’ ë¹„êµ (íš¨ìœ¨ì ì¸ ë°©ë²•)
Question: "PDU 4ë‘ 5ì˜ í˜„ì¬ ì „ë¥˜ ê°’ ë¹„êµí•´ì¤˜."
MySQL Query:
USE gist_agent_test; 
SELECT (SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'PDU_4_Rack_A1-output_current' AND v.timestamp >= NOW() - INTERVAL 2 HOUR ORDER BY v.timestamp DESC LIMIT 1) AS pdu4_current, 
(SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'PDU_5_Rack_A2-output_current' AND v.timestamp >= NOW() - INTERVAL 2 HOUR ORDER BY v.timestamp DESC LIMIT 1) AS pdu5_current;

#### Example 15: ì „ë¥˜ ê°’ ë¬»ê¸°
Question : "ì–´ì œ PDU 3ì—ì„œ ì‚¬ìš©ëœ ìµœëŒ€ ì „ë¥˜ëŠ” ëª‡ Aì˜€ì–´?"
MySQL Query:
USE gist_agent_test;
SELECT MAX(v.value) AS max_current 
FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id 
WHERE o.description = 'PDU_3_RACK_A1-output_current' AND DATE(v.timestamp) = CURDATE() - INTERVAL 1 DAY;

#### Example 16: ì˜¨ë„ ë¬»ê¸°
Question : "í˜„ì¬ ë°ì´í„°ì„¼í„° í‰ê·  ì˜¨ë„ëŠ” ëª‡ ë„ì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT AVG(v.value) avg_temp
FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id
WHERE o.description LIKE 'Thermo_Hygrometer_%-temperature_ch2' AND v.timestamp >= NOW() - INTERVAL 2 HOUR AND v.value BETWEEN 0 AND 100;

#### Example 17: Specific vs. Overall Average Trend
-- Scenario A: User asks for a SPECIFIC device
Question: "ì§€ë‚œ ì£¼ 10ë²ˆ ì˜¨ìŠµë„ê³„ì˜ ìŠµë„ ì¶”ì„¸ëŠ” ì–´ë• ì–´?"
MySQL Query:
USE gist_agent_test;
SELECT
  DATE_FORMAT(v.timestamp, '%Y-%m-%d %H:00:00') AS hour_timestamp,
  AVG(v.value) AS avg_humidity
FROM fms_object_value AS v
JOIN fms_object_list AS o ON v.object_ID = o.id
WHERE o.description = 'Thermo_Hygrometer_10-humidity_ch2' 
  AND v.timestamp >= CURDATE() - INTERVAL 7 DAY AND v.value BETWEEN 0 AND 100
GROUP BY hour_timestamp
ORDER BY hour_timestamp ASC;

-- Scenario B: User asks for the GENERAL device type
Question: "ì§€ë‚œ ì£¼ ì˜¨ìŠµë„ê³„ì˜ ì „ë°˜ì ì¸ ìŠµë„ ì¶”ì„¸ëŠ” ì–´ë• ì–´?"
MySQL Query:
USE gist_agent_test;
SELECT
  DATE_FORMAT(v.timestamp, '%Y-%m-%d %H:00:00') AS hour_timestamp,
  AVG(v.value) AS overall_avg_humidity 
FROM fms_object_value AS v
JOIN fms_object_list AS o ON v.object_ID = o.id
WHERE o.description LIKE 'Thermo_Hygrometer_%-humidity_ch2'
  AND v.timestamp >= CURDATE() - INTERVAL 7 DAY AND v.value BETWEEN 0 AND 100
GROUP BY hour_timestamp 
ORDER BY hour_timestamp ASC;

#### Example 18: Comparing Multiple Devices and Metrics (Best Practice)
Question: "í•­ì˜¨í•­ìŠµê¸° 1ê³¼ 2ì˜ ì˜¤ëŠ˜ í‰ê·  ì˜¨ë„ì™€ ìŠµë„ë¥¼ ì•Œë ¤ì¤˜. ì–´ëŠ ê¸°ê¸°ê°€ ë” ì‹œì›í•˜ê²Œ ìœ ì§€ë˜ê³  ìˆì–´?"
MySQL Query:
USE gist_agent_test;
SELECT
    AVG(CASE WHEN o.description = 'Constant_Temp_and_Humi_Chamber_1-current_temperature' THEN v.value END) AS c1_temp_avg,
    AVG(CASE WHEN o.description = 'Constant_Temp_and_Humi_Chamber_1-current_humidity' THEN v.value END) AS c1_hum_avg,
    AVG(CASE WHEN o.description = 'Constant_Temp_and_Humi_Chamber_2-current_temperature' THEN v.value END) AS c2_temp_avg,
    AVG(CASE WHEN o.description = 'Constant_Temp_and_Humi_Chamber_2-current_humidity' THEN v.value END) AS c2_hum_avg
FROM fms_object_value AS v JOIN fms_object_list AS o ON v.object_ID = o.id 
WHERE o.description IN ('Constant_Temp_and_Humi_Chamber_1-current_temperature',
        'Constant_Temp_and_Humi_Chamber_1-current_humidity',
        'Constant_Temp_and_Humi_Chamber_2-current_temperature',
        'Constant_Temp_and_Humi_Chamber_2-current_humidity')
    AND DATE(v.timestamp) = CURDATE()
    AND v.value BETWEEN 0 AND 100;

#### Example 20: Checking Multiple States of a Single Device
Question: "í•­ì˜¨í•­ìŠµê¸° 2ì˜ ëƒ‰ë°© ìƒíƒœì™€ ë‚œë°© ìƒíƒœê°€ ì§€ê¸ˆ ë‘˜ ë‹¤ ì¼œì ¸ ìˆëŠ”ì§€ ì•Œë ¤ì¤„ë˜?" -- ì°¸ê³ : ìƒíƒœê°’ì—ì„œ 1ì€ 'ì¼œì§(ON)', 0ì€ 'êº¼ì§(OFF)'ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

MySQL Query:
USE gist_agent_test;
SELECT
    (SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'Constant_Temp_and_Humi_Chamber_2-running_status_coldroom' ORDER BY v.timestamp DESC LIMIT 1) AS coldroom_status,
    (SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.description = 'Constant_Temp_and_Humi_Chamber_2-running_status_warmroom' ORDER BY v.timestamp DESC LIMIT 1) AS warmroom_status;
    

#### Example 21:
Question: "ì§€ê¸ˆ ë°ì´í„°ì„¼í„°ì˜ ì£¼ìš” í™˜ê²½ ì§€í‘œì™€ ì „ë ¥ ì§€í‘œë“¤ì„ ê°„ë‹¨íˆ ì•Œë ¤ì¤˜" -- ì°¸ê³ :(ì˜ˆ: í‰ê·  ì˜¨ë„, ìŠµë„, PUE)
MySQL Query:
USE gist_agent_test;
SELECT (SELECT AVG(v.value) FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.objectName = 'current_temperature') AS avg_temp_all, (SELECT AVG(v.value) FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.objectName = 'current_humidity') AS avg_hum_all, (SELECT v.value FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id WHERE o.objectName = 'PUE' ORDER BY v.timestamp DESC LIMIT 1) AS current_PUE;

#### Example 22: Comparing Multiple Devices - "()" ê´„í˜¸ ì‚¬ìš©!
Question : " ëª¨ë“  í•­ì˜¨í•­ìŠµê¸°ì˜ ì§€ë‚œ 3ì¼ê°„ í‰ê·  ì˜¨ë„ì™€ ìŠµë„ë¥¼ ì•Œë ¤ì¤˜." 
MySQL Query:
USE gist_agent_test;
SELECT
  AVG(CASE WHEN o.description LIKE 'Constant_Temp_and_Humi_Chamber_%-current_temperature' THEN v.value END) AS average_temperature_celsius,
  AVG(CASE WHEN o.description LIKE 'Constant_Temp_and_Humi_Chamber_%-current_humidity' THEN v.value END) AS average_humidity_percent
FROM fms_object_value AS v
JOIN fms_object_list AS o
  ON v.object_ID = o.id
WHERE
  (o.description LIKE 'Constant_Temp_and_Humi_Chamber_%-current_temperature' OR o.description LIKE 'Constant_Temp_and_Humi_Chamber_%-current_humidity')
  AND v.timestamp >= CURDATE() - INTERVAL 3 DAY
  AND v.value BETWEEN 0 AND 100;

#### Example 23: Querying a Specific Distribution Board Panel
Question: "ë¶„ì „ë°˜ 4ë²ˆ LP-AC2 íŒ¨ë„ì˜ í˜„ì¬ ì—­ë¥ ì€ ì–¼ë§ˆì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT
  v.value
FROM fms_object_value AS v
JOIN fms_object_list AS o
  ON v.object_ID = o.id
WHERE
  o.description = 'Distribution_Board_4_LP_AC2_Panel-power_factor'
ORDER BY
  v.timestamp DESC
LIMIT 1;

#### Example 24 : Calculating Total Power Consumption using a Semantic Group (Advanced)
Question : "í˜„ì¬ ë°ì´í„°ì„¼í„° ì „ì²´ ì „ë ¥ ì†Œë¹„ëŸ‰ì€ ëª‡ kW ì •ë„ì•¼?"
MySQL Query:
USE gist_agent_test;
SELECT SUM(latest_power_values.value / 1000) total_current_power_kW FROM (SELECT v.value, ROW_NUMBER() OVER (PARTITION BY o.id ORDER BY v.timestamp DESC) rn 
FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id 
WHERE (o.description LIKE 'PDU_%-output_power' or o.description like 'Distribution_Board_%-power' or o.description like 'Bus_Duct_%-power' or o.description like 'Chamber_Power_Meter_%-active_power' or o.description = 'Post_processing_data-Server_power') 
AND v.value IS NOT NULL AND v.value >= 0 AND v.timestamp >= now() - interval 1 DAY) as latest_power_values 
WHERE latest_power_values.rn = 1;

#### Example 25 : Hot spot question
Question : "ì–´ì œ(ìµœê·¼) í•«ìŠ¤íŒŸ ë°œìƒí–ˆì–´?"
MySQL Query:
USE gist_agent_test;
SELECT v.timestamp, v.vlaue 
FROM fms_object_value v JOIN fms_object_list o ON v.object_ID = o.id
WHERE o.objectName = 'AB_hot_average_temperature' AND v.value > 35 AND DATE(v.timestamp) = CURDATE() - INTERVAL 1 DAY 
ORDER BY v.timestamp DESC;

Question: {user_question}
### MySQL Query:
"""

# Python  ì½”ë“œ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
python_gen_prompt_template = """### Instruction:
You are a Python expert specializing in pandas, matplotlib, and file operations.
Based on the user's request, generate a single, executable block of Python code to perform the task.

### CRITICAL RULE:
- A pandas DataFrame named `df` has ALREADY been loaded with all necessary data.
- You MUST use this `df` variable.
- **DO NOT, under any circumstances, connect to a database, read a file, or create your own dummy data.** Your entire script must assume `df` exists and work with it. Any code that tries to load data will be rejected.

### Context for the plot:
- The user wants to visualize the data based on this request: "{description}"
- The available columns in the `df` are: {df_columns}

### Your Task:
Write a Python script that takes the existing `df` and creates a plot according to the user's request and the styling guidelines.


### Design & Style Guidelines:

# [MANDATORY] You MUST include the following code block at the beginning of your script to set up a Korean font.
# You MUST then use the `font_prop` variable for all text elements to ensure Korean characters are displayed correctly.
""
# --- í•œê¸€ í°íŠ¸ ì„¤ì • (ê·¸ë˜í”„ ì½”ë“œì— ë°˜ë“œì‹œ í¬í•¨) ---
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

try:
    # Windows í™˜ê²½ì—ì„œ 'ë§‘ì€ ê³ ë”•' í°íŠ¸ ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •
    font_path = 'c:/Windows/Fonts/malgun.ttf'
    font_prop = fm.FontProperties(fname=font_path, size=12)
    plt.rc('font', family='Malgun Gothic')
except FileNotFoundError:
    # 'ë§‘ì€ ê³ ë”•'ì´ ì—†ëŠ” ê²½ìš°, ë‚˜ëˆ”ê³ ë”•ìœ¼ë¡œ ëŒ€ì²´ ì‹œë„ (ì£¼ë¡œ Linux/Mac)
    try:
        font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
        font_prop = fm.FontProperties(fname=font_path, size=12)
        plt.rc('font', family='NanumGothic')
    except FileNotFoundError:
        # ì–´ë–¤ í°íŠ¸ë„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°, ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
        print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
        font_prop = fm.FontProperties(size=12) # ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë³¸ í°íŠ¸ ì†ì„±

plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
# --- í°íŠ¸ ì„¤ì • ë ---
""

# [CRITICAL] Apply the `font_prop` to ALL text elements like this:
# - plt.title("ê·¸ë˜í”„ ì œëª©", fontproperties=font_prop)
# - plt.xlabel("Xì¶• ë ˆì´ë¸”", fontproperties=font_prop)
# - plt.ylabel("Yì¶• ë ˆì´ë¸”", fontproperties=font_prop)
# - plt.legend(prop=font_prop)  <-- Note: legend uses 'prop'
# - plt.xticks(fontproperties=font_prop)
# - plt.yticks(fontproperties=font_prop)

# [READABILITY] To prevent X-axis labels from overlapping:
# 1. Rotate the labels: `plt.xticks(rotation=45, ha='right')`
# 2. Use automatic date locators for time-series data to show fewer ticks if needed.
#    (e.g., from matplotlib.dates import DayLocator)
# 3. Always call `plt.tight_layout()` at the end to adjust spacing.

# ### Rules:
# 1. Assume the necessary libraries like pandas (as pd), matplotlib.pyplot (as plt), and datetime are imported.
# 2. If the data is already provided as 'df' DataFrame, use it directly. 
# 3. **For database access, ALWAYS use pymysql with these parameters**:
#    ```
#    conn = pymysql.connect(
#        host='192.168.0.242',
#        user='asi_agent',
#        password='agent@asi',
#        database='gist_agent_test',
#        charset='utf8mb4'
#    )
#    ```
# 4. **For PUE data, use this SQL query**:
#    ```sql
#    SELECT v.timestamp, v.value as PUE 
#    FROM fms_object_value v 
#    JOIN fms_object_list o ON v.object_ID = o.id 
#    WHERE o.objectName = 'PUE' 
#    AND v.timestamp >= DATE_SUB(NOW(), INTERVAL <N> DAY)
#    ORDER BY v.timestamp
#    ```
#    Replace <N> with the number of days needed.
# 5. **For PDU power data, use this query pattern**:
#    SELECT v.timestamp, v.value as power, o.description
#    FROM fms_object_value v 
#    JOIN fms_object_list o ON v.object_ID = o.id 
#    WHERE o.description LIKE 'PDU_%_Rack_%-output_power'
#    AND v.timestamp >= DATE_SUB(NOW(), INTERVAL <N> DAY)
#    ORDER BY v.timestamp, o.description
# 6. **For processing PDU power data by rack, use this pattern**:
#    # Extract rack information
#    df['rack'] = df['description'].str.extract(r'(Rack_[A-Za-z0-9]+)')[0]
   
#    # Create pivot table
#    pivot_df = df.pivot_table(
#        index='timestamp', 
#        columns='rack',
#        values='power',
#        aggfunc='mean'
#    )
   
#    # Resample to desired time interval
#    df_resampled = pivot_df.resample('2H').mean()
# 7. Always save plots with a descriptive filename:
#     timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
#     filename = f"plot_name_{{timestamp_str}}.png"
#     plt.savefig(filename)
#     print(f"ê·¸ë˜í”„ë¥¼ '{{filename}}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
# 8. Always display the plot using plt.show()
# 9. Ensure the code is a single, executable block and uses utf-8 encoding for file I/O.
# 10. NEVER use simulated or random data when real data can be obtained from the database.

### Conversation History:
{conversation_history}

Request: {user_question}
### Python Code:
"""

# --- 2. í—¬í¼ í•¨ìˆ˜ ë° ë„êµ¬(Tool) ì •ì˜ ---
# 07-04 í† í° ì¸¡ì • í•¨ìˆ˜ ì¶”ê°€
# [NEW] ì„¸ì…˜ ë™ì•ˆì˜ ì´ í† í° ì‚¬ìš©ëŸ‰ì„ ê¸°ë¡í•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜

token_tracker = {
    "total_prompt_tokens": 0,
    "total_completion_tokens": 0,
    "total_tokens": 0
}

def call_gemini_with_token_tracking(prompt: str, purpose: str, file_logger: logging.Logger) -> str:
    """
    Gemini APIë¥¼ í˜¸ì¶œí•˜ê³ , í† í° ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•˜ì—¬ ë¡œê¹…í•œ í›„, í…ìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ë˜í¼ í•¨ìˆ˜
    """
    try:
        response = gemini_model.generate_content(prompt)

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° ê¸°ë¡
        prompt_tokens = response.usage_metadata.prompt_token_count  # ì…ë ¥ì— ì‚¬ìš©ë˜ëŠ” í† í° ìˆ˜
        completion_tokens = response.usage_metadata.candidates_token_count  # Geminiê°€ ìƒì„±í•œ ë‹µë³€(ì¶œë ¥)ì— ì‚¬ìš©ëœ í† í° ìˆ˜
        total_tokens = response.usage_metadata.total_token_count    # ìœ„ ì…ë ¥ + ì¶œë ¥ í† í° í•©ê³„ 

        log_message = (
            f"ğŸª™ Token Usage ({purpose}): "
            f"Input={prompt_tokens}, Output={completion_tokens}, Total={total_tokens}"
        )
        file_logger.info(log_message)

        # ì „ì—­ íŠ¸ë˜ì»¤ì— í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
        token_tracker["total_prompt_tokens"] += prompt_tokens
        token_tracker["total_completion_tokens"] += completion_tokens
        token_tracker["total_tokens"] += total_tokens

        return response.text
    
    except Exception as e:
        error_message = f"âŒ Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ({purpose}): {e}"
        file_logger.error(error_message)
        # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ, ë¹ˆ í…ìŠ¤íŠ¸ë‚˜ ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        return f"API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"


def extract_code(llm_output: str) -> str:
    """LLMì˜ ë‹¤ì–‘í•œ ì¶œë ¥ í˜•ì‹ì— ëŒ€ì‘í•˜ì—¬ ì½”ë“œ ë˜ëŠ” JSON ë¸”ëŸ­ì„ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r"```(?:python|sql|json)?\s*(.*?)```", llm_output, re.DOTALL) # LLM ì¶œë ¥ì—ì„œ python, sql, json ë§¤ì¹­, DOTALLì€ ì¤„ë°”ê¿ˆ í¬í•¨í•˜ì—¬ ì „ì²´ í…ìŠ¤íŠ¸ ë§¤ì¹­
    if match:
        return match.group(1).strip()

    json_match = re.search(r'\[.*\]|\{.*\}', llm_output, re.DOTALL) # JSON í˜•ì‹ì˜ ë°ì´í„° ([] or {}) ì¶”ì¶œ
    if json_match:
        return json_match.group(0).strip()
        
    keywords = ["### Python Code:", "### MySQL Query:"]
    for keyword in keywords:
        if keyword in llm_output:
            return llm_output.split(keyword, 1)[1].strip()

    return llm_output.strip()

def execute_sql(sql_query: str) -> pd.DataFrame:
    """SQLì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    db_to_use = "gist_agent_test"
    
    # mysql í‚¤ì›Œë“œ ì œê±°
    if sql_query.lower().startswith("mysql"):
        sql_query = sql_query[5:].strip()   # mysql SELECT * FROM table ~ ì—ì„œ mysql ì œê±° í›„ ì•,ë’¤ ê³µë°± ì œê±°
    
    use_match = re.search(r"USE\s+`?(\w+)`?;", sql_query, re.IGNORECASE)    # re.IGNOREDCASEëŠ” ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ
    if use_match:
        db_to_use = use_match.group(1)  # group(1)ì€ USE ë’¤ì˜ gist_agent_testë¥¼ ë°˜í™˜
        sql_query = re.sub(r"USE\s+`?(\w+)`?;", "", sql_query, flags=re.IGNORECASE).strip() # subë¡œ sqlì¿¼ë¦¬ì—ì„œ USE êµ¬ë¬¸ ì œê±°
    
    # WITH ROLLUP êµ¬ë¬¸ ì œê±° (WITH ROLLUP - MySQLì—ì„œ ê·¸ë£¹í™”ëœ ë°ì´í„°ì˜ ìš”ì•½ì„ ê³„ì‚°í•˜ëŠ”ë°, ë” ë³µì¡í•´ì§ˆ ê¹Œë´ ì œê±°)
    sql_query = re.sub(r"GROUP BY\s+(.+?)\s+WITH\s+ROLLUP", r"GROUP BY \1", sql_query, flags=re.IGNORECASE)
    
    # ë¹„í˜¸í™˜ COALESCE íŒ¨í„´ ìˆ˜ì • - LLMì´ COALESCE ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì–´ì„œ ë¯¸ë¦¬ ì œê±°. ì¼ë¶€ MySQLì—ì„œ ì§€ì›ì•ˆí•´ì„œ.
    if "COALESCE(DATE(v.timestamp)" in sql_query:
        sql_query = sql_query.replace(
            "COALESCE(DATE(v.timestamp), 'Overall Average') AS trend_date",
            "DATE(v.timestamp) AS trend_date"
        )
    
    print(f"ì‹¤í–‰í•  SQL ì¿¼ë¦¬: {sql_query}")
    print(f"ì ‘ì†í•  DB: {db_to_use}")
    
    try:
        conn = pymysql.connect(
            host=DB_HOST, port=DB_PORT, user=DB_USER, password=DB_PASSWORD,
            database=db_to_use, charset="utf8mb4", cursorclass=pymysql.cursors.DictCursor
        )
        
        # ì»¤ì„œ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰ 
        try:
            with conn.cursor() as cursor:
                cursor.execute(sql_query)
                results = cursor.fetchall()
            
            # ê²°ê³¼ê°€ ìˆìœ¼ë©´ DataFrameìœ¼ë¡œ ë³€í™˜
            if results:
                df = pd.DataFrame(results)
                print(f"ì¿¼ë¦¬ ê²°ê³¼: {len(df)} í–‰ ë°˜í™˜ë¨")
                conn.close()
                return df
            else:
                print("ì¿¼ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                conn.close()
                return pd.DataFrame()
                
        except Exception as e1:
            print(f"ì»¤ì„œë¡œ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e1}")
            
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ pandas ë°©ì‹ ì‹œë„ 
            try:
                safe_sql_query = sql_query
                df = pd.read_sql_query(safe_sql_query, conn)    # read_sql_query ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ SQL ì¿¼ë¦¬ ì‚¬ìš© ê°€ëŠ¥.
                print(f"pandasë¡œ ì¿¼ë¦¬ ê²°ê³¼: {len(df)} í–‰ ë°˜í™˜ë¨")
                conn.close()
                return df
            except Exception as e2:
                print(f"pandasë¡œ ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e2}")
                conn.close()
                return pd.DataFrame()
    except Exception as e:
        print(f"âŒ SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def execute_python(python_code: str, exec_globals: dict = {}) -> str:
    """Python ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³ , ë™ì  ë³€ìˆ˜(DataFrame ë“±)ë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤."""
    output_buffer = io.StringIO()   # Python ì½”ë“œ ì‹¤í–‰ ì¤‘ ìƒì„±ëœ ì¶œë ¥(print ê°™ì€ê±°)ì„ ì €ì¥í•  ë²„í¼ë¥¼ ìƒì„±.
    # ìì£¼ ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê¸°ë³¸ ì „ì—­ ë³€ìˆ˜(default_globals)ë¡œ ì„¤ì •
    try:
        default_globals = {
            "pd": pd, "plt": plt, "os": os, "np": np, 
            "datetime": datetime, "timedelta": timedelta, "io": io, "re": re
        }
        combined_globals = {**default_globals, **exec_globals}
        with redirect_stdout(output_buffer):
            exec(python_code, combined_globals)
        
        output = output_buffer.getvalue()   # output_bufferì— ì €ì¥ëœ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ê°€ì ¸ì˜´.

        if 'plt.savefig' in python_code:
            match = re.search(r"plt\.savefig\(['\"](.*?)['\"]\)", python_code)
            if match and os.path.exists(match.group(1)):
                output += f"\nâœ… '{match.group(1)}' íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        return output if output else "ì½”ë“œê°€ ì‹¤í–‰ë˜ì—ˆìœ¼ë‚˜, ë³„ë„ì˜ ì¶œë ¥ì€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"âŒ Python ì½”ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}\n{output_buffer.getvalue()}"

# ìˆ˜ì •ëœ ë²”ìš©ì ì¸ ì´ìƒì¹˜ íƒì§€ í•¨ìˆ˜
def detect_anomalies(df: pd.DataFrame, metric_col: str, group_by_col: str) -> pd.DataFrame:
    """
    ì£¼ì–´ì§„ ë°ì´í„°í”„ë ˆì„ì—ì„œ íŠ¹ì • ì¸¡ì •ê°’(metric)ì˜ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” 'ë²”ìš©' ë„êµ¬ í•¨ìˆ˜
    :param df: ë¶„ì„í•  ì›ë³¸ ë°ì´í„°í”„ë ˆì„
    :param metric_col: ì´ìƒì¹˜ë¥¼ íƒì§€í•  ì¸¡ì •ê°’ ì»¬ëŸ¼ ì´ë¦„ (ì˜ˆ: 'power', 'average_temperature_celsius')
    :param group_by_col: ê·¸ë£¹í™”í•  ê¸°ì¤€ ì»¬ëŸ¼ ì´ë¦„ (ì˜ˆ: 'deviceName', 'measurement_date')
    """
    # í•¨ìˆ˜ê°€ ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ì„ ë°›ì•˜ëŠ”ì§€ í™•ì¸
    if metric_col not in df.columns or group_by_col not in df.columns:
        return pd.DataFrame() 

    # 'power', 'deviceName' ëŒ€ì‹  ì¸ìë¡œ ë°›ì€ ì»¬ëŸ¼ ì´ë¦„ì„ ì‚¬ìš©
    stats = df.groupby(group_by_col)[metric_col].agg(['mean', 'std']).reset_index()
    df_with_stats = pd.merge(df, stats, on=group_by_col)
    
    # NaN ê°’ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ fillna(0) ì¶”ê°€ (í‘œì¤€í¸ì°¨ê°€ 0ì¸ ê²½ìš° NaNì´ ë  ìˆ˜ ìˆìŒ)
    df_with_stats['std'] = df_with_stats['std'].fillna(0)

    # ì´ìƒì¹˜ ê³„ì‚°
    df_with_stats['is_anomaly'] = df_with_stats[metric_col] > (df_with_stats['mean'] + 2 * df_with_stats['std'])
    
    anomalies = df.loc[df_with_stats[df_with_stats['is_anomaly']].index]
    
    # ê²°ê³¼ê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ìš”ì•½ ë¡œì§ ì‹¤í–‰
    if not anomalies.empty:
        anomaly_summary = anomalies.loc[anomalies.groupby(group_by_col)[metric_col].idxmax()]
        # ë°˜í™˜í•˜ëŠ” ì»¬ëŸ¼ë„ ë™ì ìœ¼ë¡œ ì„ íƒ
        return anomaly_summary[[group_by_col, metric_col, 'timestamp']]
    else:
        return pd.DataFrame()

# --- ë©€í‹°ìŠ¤í…ì„ ìœ„í•œ ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ë“¤ --- 0704 í† í° ì¶œë ¥ë•Œë¬¸ì— ìˆ˜ì •
def get_plan(user_question: str, file_logger: logging.Logger) -> List[Dict]:    # file_loggerë¥¼ ì¸ìë¡œ ë°›ë„ë¡ ìˆ˜ì •
    """Plannerë¥¼ í˜¸ì¶œí•˜ì—¬ í–‰ë™ ê³„íšì„ JSONìœ¼ë¡œ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
    prompt = planner_prompt_template.format(user_question=user_question)
    # ìƒˆ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³ , ëª©ì ì„ 'Planner'ë¡œ ëª…ì‹œ
    response_text = call_gemini_with_token_tracking(prompt, "Planner", file_logger)
    plan_str = extract_code(response_text)  # plan_strì€ JSON í˜•ì‹ì˜ ë¬¸ìì—´ (ì˜ˆ:[{ "tool": "db_querier", "description":"ì´ë²ˆì£¼~ì¡°íšŒ"}])
    return json.loads(plan_str)     # json.loads ëŠ” JSON ë¬¸ìì—´ì„ Python ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    # loads ëŠ” 'load string'ì˜ ì•½ìë¡œ, ë¬¸ìì—´ í˜•íƒœì˜ JSON ë°ì´í„°ë¥¼ Python ê°ì²´ë¡œ 'ë¡œë“œ' í•œë‹¤ëŠ” ì˜ë¯¸.

# 0704 ê¸°ë¡ ê´€ë ¨ ê¸°ì–µë•Œë¬¸ì— ìˆ˜ì •
def generate_sql_code(description: str, conversation_history: List[Dict], file_logger: logging.Logger) -> str:  # file_logger ì¸ì ìœ ì§€
    """SQL ì½”ë“œ ìƒì„±ì„ ì „ë‹´í•˜ëŠ” í•¨ìˆ˜(ë‘ ì¢…ë¥˜ì˜ RAG ëª¨ë‘ í™œìš©)"""
    # [ìˆ˜ì •] SQL ìƒì„± ì‹œì—ëŠ” ê³¼ê±° ëŒ€í™” ê¸°ë¡ì„ ì°¸ì¡°í•˜ì§€ ì•Šì•„ LLMì˜ í˜¼ë€ì„ ë°©ì§€í•©ë‹ˆë‹¤. (RAGì™€ ëŒ€í™”ê¸°ë¡ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.)
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])
    
    # [ìˆ˜ì •] ë‘ ì¢…ë¥˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ê²€ìƒ‰
    retrieved_terms = retrieve_sql_context(description) # deviceName RAG
    retrieved_qa_examples = retrieve_qa_examples(description)   # Q&A Set RAG ìƒˆë¡œ ì¶”ê°€

    current_date_str = datetime.now().strftime("%Y-%m-%d")
    
    # [ìˆ˜ì •] í”„ë¡¬í”„íŠ¸ì— ë‘ ì»¨í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì „ë‹¬
    prompt = sql_gen_prompt_template.format(
        conversation_history="", # <-- ì´ ë¶€ë¶„ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€ê²½
        retrieved_context=retrieved_terms,
        retrieved_qa_examples=retrieved_qa_examples, # ìƒˆë¡œ ì¶”ê°€
        user_question=description,
        current_date=current_date_str
    )

    # ì¤‘ê°„ í™•ì¸ì„ ìœ„í•´ ë¡œê·¸ ì¶œë ¥
    print("[RAG Q&A ì˜ˆì‹œ]\n", retrieved_qa_examples)
    print("[ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¼ë¶€]\n", prompt[:1000])

    response_text = call_gemini_with_token_tracking(prompt, "SQL_Generator", file_logger)    # ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ
    return extract_code(response_text)

def generate_viz_code(description: str, df: pd.DataFrame, conversation_history: list, file_logger: logging.Logger) -> str:    
    """ì‹œê°í™” ì½”ë“œ ìƒì„±ì„ ì „ë‹´í•˜ëŠ” í•¨ìˆ˜"""  
    # ëŒ€í™” ê¸°ë¡ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ì‚¬ìš© - descriptionì€ ì‹œê°í™”ì— ëŒ€í•œ ì„¤ëª… "~ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ì¤˜"
    history_str = ""
    if conversation_history:
        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])
    
    prompt = python_gen_prompt_template.format(
        df_columns=str(df.columns.to_list()),
        description=description,
        user_question=description,  # python_gen_prompt_templateì—ì„œ í…œí”Œë¦¿ ìœ ì—°ì„±ì„ ìœ„í•´ descriptionì„ 2ê°œë¡œ ì‚¬ìš©
        conversation_history=history_str
    )
    # 07-07 ìˆ˜ì • ëª¨ë‘ call~token ë³€ìˆ˜ë¡œ
    response_text = call_gemini_with_token_tracking(prompt, "Visualizer_Code_Generator", file_logger)
    return extract_code(response_text)

# 0701 
def summarize_table(df: pd.DataFrame) -> str:
    """ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼(ì„¼ì„œê°’, ì „ë ¥, ì˜¨ë„ ë“±)ì— ëŒ€í•´ ì£¼ìš” ë³€í™”/ë³€ë™í­ ìš”ì•½"""
    if df.empty:
        return "ë³€ë™ ìš”ì•½ì„ í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìë™ íƒìƒ‰
    num_cols = df.select_dtypes(include=['number']).columns.tolist()
    if not num_cols:
        return "ìˆ˜ì¹˜í˜•(ìˆ«ìí˜•) ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ë³€ë™ ìš”ì•½ ë¶ˆê°€."
    # ë°ì´í„° ì „ì²´ í–‰ì— ëŒ€í•´ ID/ì´ë¦„/ì¥ì¹˜/ì„¤ë¹„/ì„¼ì„œëª… ì»¬ëŸ¼ ì°¾ê¸°
    id_cols = [c for c in df.columns if any(k in c.lower() for k in ["pdu", "id", "sensor", "chamber", "name", "description"])]
    lines = []
    for col in num_cols:
        col_min = df[col].min()
        col_max = df[col].max()
        col_mean = df[col].mean()
        col_std = df[col].std()
        col_range = col_max - col_min
        lines.append(f"- '{col}'ì˜ ìš”ì•½: min={col_min:.2f}, max={col_max:.2f}, mean={col_mean:.2f}, std={col_std:.2f}, ë³€ë™í­={col_range:.2f}")
        # ID ë³„ ê° ì¥ì¹˜/ì„¼ì„œ/ê·¸ë£¹ë³„ë¡œ ìƒìœ„ 5ê°œ ë³€ë™í­ ìš”ì•½
        for id_col in id_cols:
            group_stats = df.groupby(id_col)[col].agg(['min', 'max', 'mean', 'std'])
            group_stats['range'] = group_stats['max'] - group_stats['min']
            top_var = group_stats.sort_values('range', ascending=False).head(5)
            lines.append(f"  {id_col}ë³„ '{col}' ë³€ë™í­ TOP5:")
            for idx, row in top_var.iterrows():
                lines.append(f"    â€¢ {idx}: min={row['min']:.1f}, max={row['max']:.1f}, mean={row['mean']:.1f}, std={row['std']:.1f}, ë³€ë™í­={row['range']:.1f}")
    return "\n".join(lines)

def synthesize_answer_from_summary(user_question: str, conversation_history: list, summary: str, file_logger: logging.Logger) -> str:
    """
    ìš”ì•½(summary) í…ìŠ¤íŠ¸ë§Œ ê°€ì§€ê³  LLMì´ ìì—°ì–´ ë‹µë³€ ìƒì„±.
    """
    history_str = "\n".join([f"## {msg['role']}:\n{msg['content']}" for msg in conversation_history])
    prompt = f"""### Instruction:
ë„ˆëŠ” ë°ì´í„° ë¶„ì„ AIì•¼. ì•„ë˜ summaryë§Œ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µí•´. ì¶”ì¸¡ì´ë‚˜ ê·¼ê±° ì—†ëŠ” í™•ì¥ ì„¤ëª…ì€ ê¸ˆì§€. summaryì˜ ìˆ«ì/í•­ëª©ë§Œ ì¸ìš©.

### Context:
- ì‚¬ìš©ì ì§ˆë¬¸: "{user_question}"
- ëŒ€í™” ê¸°ë¡:
{history_str}
- ë°ì´í„° ìš”ì•½(summary):
{summary}

### ìµœì¢… ë‹µë³€(í•œêµ­ì–´):
"""
    # ì¼ê´€ì„±ì„ ìœ„í•´ ë˜í¼ í•¨ìˆ˜ ì‚¬ìš©
    return call_gemini_with_token_tracking(prompt, "Synthesize_from_Summary", file_logger)
# 07-09 17:16 ìˆ˜ì •
# sql_query ì¸ì ì¶”ê°€
def synthesize_answer_from_dataframe(user_question: str, conversation_history: list, df: pd.DataFrame, sql_query: str, file_logger: logging.Logger) -> str:
    """
    DataFrameê³¼ ì´ë¥¼ ìƒì„±í•œ SQL ì¿¼ë¦¬ê¹Œì§€ í•¨ê»˜ LLMì— ì œê³µí•˜ì—¬, ë” ë†’ì€ í’ˆì§ˆì˜ ìì—°ì–´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if df.empty:
        # ì´ì „ì— 1ë‹¨ê³„ì—ì„œ "ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"ì™€ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì™”ì„ ë•Œ, 
        # ìµœì¢… ë‹µë³€ ìƒì„± ë‹¨ê³„ì—ì„œ ì´ ë©”ì‹œì§€ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    if len(df) > 20:
        table_str = df.head(20).to_markdown(index=False) + "\n(ì´í•˜ ìƒëµ...)"
    else:
        table_str = df.to_markdown(index=False)

    history_str = "\n".join([f"## {msg['role']}:\n{msg['content']}" for msg in conversation_history])
    
    prompt = f"""### Instruction:
ë„ˆëŠ” ì „ë¬¸ ë°ì´í„° ë¶„ì„ê°€ AIì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼, ê·¸ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì‹¤í–‰ëœ SQL ì¿¼ë¦¬, ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ ë°ì´í„°ë¥¼ ë³´ê³  ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´.

### Context:
- **ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸**: "{user_question}"

- **ì‹¤í–‰ëœ SQL ì¿¼ë¦¬**:
```sql
{sql_query}
- **ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ ë°ì´í„°**:
{table_str}

### ìµœì¢… ë‹µë³€(í•œêµ­ì–´):
ìœ„ì˜ ëª¨ë“  ë§¥ë½ì„ ì¢…í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•´ì¤˜. ì¿¼ë¦¬ì— ìˆëŠ” ì¡°ê±´ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì™œ ì´ ë°ì´í„°ê°€ ë‚˜ì™”ëŠ”ì§€ ì„¤ëª…ì— í¬í•¨í•  ìˆ˜ ìˆì–´. 

"""
    # ì¼ê´€ì„±ì„ ìœ„í•´ ë˜í¼ í•¨ìˆ˜ ì‚¬ìš©
    return call_gemini_with_token_tracking(prompt, "Synthesize_from_DataFrame", file_logger)
# 07-06 report_generator agent

def generate_report(user_question: str, execution_context: dict, conversation_history: list, plan: list) -> str:
    """
    ì´ì „ ë‹¨ê³„ë“¤ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°(í…Œì´ë¸”, ê·¸ë˜í”„ ë“±)ë¥¼ ì¢…í•©í•˜ì—¬
    êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´(Markdown) ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    
    # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë³´ê³ ì„œì— í¬í•¨í•  ëª¨ë“  ê²°ê³¼ë¬¼ì„ ìˆ˜ì§‘
    report_elements = []
    for key, value in execution_context.items():
        if key.startswith('output_of_step_'):
            step_num = key.split('_')[-1]
            step_plan = plan[int(step_num) - 1] # í•´ë‹¹ ë‹¨ê³„ì˜ ê³„íš ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            header = f"### [ë¶„ì„ {step_num}: {step_plan['tool']} - {step_plan['description']}]"
            
            element_body = ""
            if isinstance(value, pd.DataFrame):
                element_body = value.to_markdown(index=False) if not value.empty else "ê²°ê³¼ ë°ì´í„° ì—†ìŒ."
            elif "íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤" in str(value):
                # ì‹œê°í™” ê²°ê³¼ì—ì„œ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
                match = re.search(r"'([^']+\.png)'", str(value))
                if match:
                    filepath = match.group(1)
                    element_body = f"ìƒì„±ëœ ê·¸ë˜í”„: {filepath}"
            else:
                element_body = str(value)
            
            report_elements.append(f"{header}\n{element_body}")

    combined_context = "\n\n---\n\n".join(report_elements)
    history_str = "\n".join([f"## {msg['role']}:\n{msg['content']}" for msg in conversation_history])

    # ë³´ê³ ì„œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    report_prompt = f"""### Instruction:
You are a professional data center operations analyst. Your task is to generate a formal report in Korean based on the provided data and visualizations. The report should be well-structured, easy to understand, and directly address the user's original request.

### Report Generation Context:
- User's Original Request: "{user_question}"
- Conversation History: {history_str}
- Collected Data and Analysis Results:
{combined_context}

### Report Structure:
1.  **ì œëª© (Title)**: ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ê³ ì„œì˜ ì œëª©ì„ ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: ì£¼ê°„ ë°ì´í„°ì„¼í„° ì „ë ¥ ì‚¬ìš©ëŸ‰ ë³´ê³ ì„œ).
2.  **ê°œìš” (Summary)**: ë¶„ì„ëœ í•µì‹¬ ë‚´ìš©ì„ 1~2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
3.  **ì„¸ë¶€ ë¶„ì„ ë‚´ìš© (Detailed Analysis)**: ê° ë¶„ì„ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ í•­ëª©ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•˜ì„¸ìš”. ë°ì´í„° í…Œì´ë¸”ê³¼ ê·¸ë˜í”„ ê²°ê³¼ë¥¼ ëª…í™•íˆ ì°¸ì¡°í•˜ì—¬ ì„œìˆ í•˜ì„¸ìš”.
4.  **ê²°ë¡  ë° ì œì–¸ (Conclusion & Recommendation)**: ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ë¡ ì„ ë‚´ë¦¬ê³ , í•„ìš”í•œ ê²½ìš° ìš´ì˜ìƒ ì œì–¸ì„ ë§ë¶™ì´ì„¸ìš”.

### Final Report (in Korean Markdown format):
"""
    # Geminië¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ìƒì„±
    final_report = call_gemini_with_token_tracking(report_prompt, "Report_Generator", file_logger) # logger ì „ë‹¬
    return final_report


# --- 3. ë©”ì¸ AI ì—ì´ì „íŠ¸ í•¨ìˆ˜ (Orchestrator) ---
def run_ai_agent(user_question: str, file_logger: logging.Logger, conversation_history: list):
    """(ë©€í‹°ìŠ¤í… ì•„í‚¤í…ì²˜) ê³„íšì— ë”°ë¼ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    print("\nğŸ¤” ìƒê° ì¤‘...")
    file_logger.info(f"ğŸ¤– ì‚¬ìš©ì ì§ˆë¬¸: {user_question}")
    
    final_output = ""
    intermediate_outputs = []   # ëŒ€í™” ì´ë ¥ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸(ìˆœì„œìˆì–´ì„œ) - ìµœì¢… ê²°ê³¼ ë°˜í™˜ ì „ í™•ì¸ ë° ë””ë²„ê¹…ìš©
    execution_context = {}      # DB ì¡°íšŒí•œ ê²°ê³¼ (DataFrame)ë¥¼ ì €ì¥, ì‹œê°í™”, ì´ìƒì¹˜ íƒì§€ ë‹¨ê³„ì—ì„œ ì¬ì‚¬ìš©

    try:
        # [ìˆ˜ì •]
        plan = get_plan(user_question, file_logger)
        file_logger.info(f"ğŸ§  ìˆ˜ë¦½ëœ ê³„íš: {plan}")
        print(f" ìˆ˜ë¦½ëœ ê³„íš :")
        for i, step in enumerate(plan): # í”Œë˜ë„ˆê°€ ìƒì„±í•œ ê³„íšì„ ìˆœíšŒí•˜ë©° ì‹¤í–‰(2ê°œ ì´ìƒ)
            print(f" {i+1}. {step['tool']}: {step['description']}")
        # 06-27 16:30 ìˆ˜ì • - 06-30 16:53 "result": [] ì‚­ì œ

        ## --- 1. ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„(for ë£¨í”„) --- ##
        # ì´ ë£¨í”„ì˜ ëª©í‘œëŠ” ê³„íšì— ë”°ë¼ ê° ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ execution_contextì— ì €ì¥í•˜ëŠ” ê²ƒ.
        for i, step in enumerate(plan):
            tool = step['tool']
            description = step['description']
            
            #[*ìˆ˜ì •] answer_synthesizerëŠ” ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ì´ ëë‚œ í›„ ë§ˆì§€ë§‰ì— ë”°ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ, ë£¨í”„ì—ì„œëŠ” ê±´ë„ˆëœ€.
            if tool == 'answer_synthesizer':
                continue

            print(f"--- {i+1}/{len(plan)}ë‹¨ê³„ ì‹¤í–‰: {tool} ({description}) ---")
            file_logger.info(f"â–¶ {i+1}ë‹¨ê³„ ì‹¤í–‰: {tool} - {description}")

            # ê° ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ì„ ë‹´ì„ ì¼ì‹œ ë³€ìˆ˜. ì´ ë³€ìˆ˜ì—ëŠ” DataFrame ë˜ëŠ” strì´ ë‹´ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            step_result_data = None

            # ê° ë„êµ¬ëŠ” ìì‹ ì˜ ì—­í• (ì •ë³´ ìˆ˜ì§‘)ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.
            if tool == 'db_querier':
                sql_code = generate_sql_code(description, conversation_history, file_logger)
                file_logger.info(f"ğŸ’» ìƒì„±ëœ SQL:\n---\n{sql_code}\n---")
                
                # SQL ì‹¤í–‰ê³¼ ì˜¤ë¥˜ ì²˜ë¦¬
                try:    # result_dfë¥¼ step_result_dataë¡œ ê³ ì¹¨. 
                    step_result_data = execute_sql(sql_code)
                    execution_context[f'dataframe_step_{i+1}'] = step_result_data   # ê° ë‹¨ê³„ë³„ ê²°ê³¼ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì €ì¥í•˜ì—¬ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ í•¨.
                    # 0630 16:16 ìˆ˜ì •(GPT)
                    execution_context['dataframe'] = step_result_data  # ì´ê²Œ í•µì‹¬ - ìµœì‹  ë°ì´í„°ë§Œ ì €ì¥(í•­ìƒ ë®ì–´ì“°ê¸°)

                    # ê²°ê³¼ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
                    if isinstance(step_result_data, pd.DataFrame):     # DataFrame íƒ€ì…ì¸ì§€ ê²€ì‚¬
                        if step_result_data.empty:
                            final_output = "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                        else:
                            if len(step_result_data) == 1 and len(step_result_data.columns) == 1:      # SQL ì¿¼ë¦¬ ê²°ê³¼ê°€ ë‹¨ì¼ ê°’ì¸ ê²½ìš° - row, column ìˆ˜ê°€ 1ê°œì¸ì§€ í™•ì¸    
                                # final_output = f"ë°ì´í„° ì¡°íšŒë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤: {result_df.iloc[0, 0]}"
                                the_actual_value = step_result_data.iloc[0, 0]
                                if isinstance(the_actual_value, (int, float)):
                                    final_output = f"ë°ì´í„° ì¡°íšŒë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤: {the_actual_value:.1f}"
                                else:
                                    final_output = f"ë°ì´í„° ì¡°íšŒë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤: {the_actual_value}"
                            else:
                                final_output = f"ë°ì´í„° ì¡°íšŒë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. {len(step_result_data)}ê°œì˜ í–‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n(ë¯¸ë¦¬ë³´ê¸°)\n{step_result_data.head().to_string()}"
                    
                    elif isinstance(step_result_data, str):    # SQL ì½”ë“œê°€ str íƒ€ì…ì¸ì§€ ê²€ì‚¬
                        # ë¬¸ìì—´ë¡œ ë°˜í™˜ëœ ê²½ìš° (ê¸°ì¡´ ì—ì´ì „íŠ¸ì™€ í˜¸í™˜ì„± ìœ ì§€)
                        final_output = step_result_data
                        # ë¬¸ìì—´ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì‹œë„
                        try:
                            if "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" not in step_result_data:    # 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤' ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•˜ì§€ ì•Šë„ë¡ ìœ„í•¨
                                import ast
                                data = ast.literal_eval(step_result_data)  # ë¬¸ìì—´ì„ Python ê°ì²´ë¡œ ë³€í™˜
                                execution_context[f'dataframe_step_{i+1}'] = pd.DataFrame(data) # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥
                        except:
                            pass
                    else:
                        final_output = f"ë°ì´í„° ì¡°íšŒë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ ìœ í˜•: {type(step_result_data)}"
                except Exception as e:
                    final_output = f"SQL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    file_logger.error(final_output, exc_info=True)
                
                #  ë™ì  ë¶„ê¸° ë¡œì§ ì¶”ê°€ :plan ë§ˆì§€ë§‰ ë‹¨ê³„ì¼ ë•Œë§Œ
                is_last_step = (i == len(plan) - 1) # ë‹¨ìˆœ db ë°ì´í„° íƒìƒ‰ ì¼ ë•Œ ì“°ê¸° ìœ„í•œ ë¡œì§
                plan_only_db_querier = (len(plan)==1 and tool =="db_querier")

                if is_last_step and plan_only_db_querier:
                    if isinstance(step_result_data, pd.DataFrame) and not step_result_data.empty:
                        # ì¡°ê±´ë¶€ ìš”ì•½ ë° ë‹µë³€ ìƒì„±
                        if len(step_result_data) > 100: # ê²°ê³¼ í–‰ ìˆ˜ ê¸°ì¤€, ìƒí™©ì— ë§ê²Œ ì„ê³„ê°’ ì¡°ì •
                            print("--- ë™ì  ì¶”ê°€: data_summarizer + answer_synthesizer ---")
                            # ìš”ì•½ ì‹¤í–‰(100í–‰ ì´ˆê³¼ì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„°)
                            summary = summarize_table(step_result_data)
                            # ìš”ì•½ëœ ë‚´ìš© ê¸°ë°˜ ë‹µë³€(API í˜¸ì¶œ)
                            final_output = synthesize_answer_from_summary(user_question, conversation_history, summary, file_logger)
                        else:
                            print("--- ë™ì  ì¶”ê°€: answer_synthesizer ---")
                            final_output = synthesize_answer_from_dataframe(user_question, conversation_history, step_result_data, sql_code, file_logger)
                        break   # ë™ì  ë¶„ê¸°ì—ì„œ ìµœì¢… ë‹µë³€ ë§Œë“¤ì—ˆìœ¼ë©´ forë¬¸ íƒˆì¶œ

            elif tool == 'visualizer':
                # 25-06-30 16:46 ì¶”ê°€
                df_to_visualize = execution_context.get('dataframe', pd.DataFrame())

                if df_to_visualize.empty:
                    final_output = "ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
                    break
                # 07-07 ìˆ˜ì •       
                python_code = generate_viz_code(description, df_to_visualize, conversation_history, file_logger)
                file_logger.info(f"ğŸ’» ìƒì„±ëœ Python Code:\n---\n{python_code}\n---")
                step_result_data = execute_python(python_code, exec_globals={'df': df_to_visualize})
            
            elif tool == 'anomaly_detector':
                # 25-06-30 16:46 ì¶”ê°€ 07-08 16:40 ë³€ê²½
                source_df = execution_context.get('dataframe', pd.DataFrame())
                if source_df.empty:
                    # 0704 15:53 ìˆ˜ì •. ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´, ë¹ˆ ë°ì´í„°í”„ë ˆì„ì„ ê²°ê³¼ë¡œ ì„¤ì •
                    step_result_data = pd.DataFrame()
                else:
                    # 07-08 ë” ë‹¤ì–‘í•œ ë¶„ì„ ë¡œì§
                    metric_col = None
                    group_by_col = None

                    # 1. ê·¸ë£¹í™”í•  ì»¬ëŸ¼(group_by_col) ì°¾ê¸° (ì˜ˆ: deviceName, description, measurement_date ë“±)
                    possible_group_cols = ['deviceName', 'description', 'measurement_date', 'name', 'id']
                    for col in possible_group_cols:
                        if col in source_df.columns:
                            group_by_col = col
                            break
                    
                    # 2. ë¶„ì„í•  ì¸¡ì •ê°’ ì»¬ëŸ¼(metric_col) ì°¾ê¸° (ìˆ«ìí˜•ì´ê³ , ê·¸ë£¹ ì»¬ëŸ¼ì´ë‚˜ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì•„ë‹Œ ì»¬ëŸ¼)
                    for col in source_df.columns:
                        if pd.api.types.is_numeric_dtype(source_df[col]) and col not in possible_group_cols and 'timestamp' not in col:
                            metric_col = col
                            break
                    
                    # 3. ë¶„ì„ ëŒ€ìƒ ì»¬ëŸ¼ì„ ì°¾ì•˜ìœ¼ë©´, ë²”ìš© í•¨ìˆ˜ í˜¸ì¶œ
                    if metric_col and group_by_col:
                        print(f"ğŸ”¬ ì´ìƒì¹˜ ë¶„ì„ ì‹¤í–‰: ê·¸ë£¹({group_by_col}), ì¸¡ì •ê°’({metric_col})")
                        anomalies_df = detect_anomalies(source_df, metric_col, group_by_col)
                        step_result_data = anomalies_df
                    else:
                        # ë¶„ì„ ëŒ€ìƒì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
                        print("âš ï¸ ì´ìƒì¹˜ ë¶„ì„ì„ ìœ„í•œ ì ì ˆí•œ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                        step_result_data = pd.DataFrame()
                    # --- ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ë¡œì§ ë ---
                
                # ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥
                execution_context['dataframe'] = step_result_data
                    
                
            
            # ë°ì´í„° ìš”ì•½ ì¶”ê°€ 0701-10:06 ìˆ˜ì¹˜ ë°ì´í„°ë“¤ì— ëŒ€í•œ í…ìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
            elif tool == 'data_summarizer':
                # [ìˆ˜ì •]  ìš”ì•½ í…ìŠ¤íŠ¸ ê²°ê³¼ë§Œ ë°˜í™˜
                source_df = execution_context.get('dataframe', pd.DataFrame())
                if source_df.empty:
                    step_result_data = 'ìš”ì•½í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'
                else:
                    summary_text = summarize_table(source_df)
                    #ìš”ì•½ ê²°ê³¼ë¥¼ step_result_dataì— í• ë‹¹
                    step_result_data = summary_text
 
            elif tool == 'memory_retriever':
                # [ì„¤ëª…] memory_retrieverëŠ” ê·¸ ìì²´ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” 'ì™„ê²°ëœ' ë„êµ¬ì…ë‹ˆë‹¤.
                # ë”°ë¼ì„œ ì´ ë„êµ¬ê°€ ë§Œë“  ê²°ê³¼ê°€ ì‚¬ì‹¤ìƒ ìµœì¢… ë‹µë³€ì´ ë©ë‹ˆë‹¤.
                
                if not conversation_history or len(conversation_history) < 2:
                    step_result_data = "ì°¸ì¡°í•  ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
                else:
                    # ê°€ì¥ ìµœê·¼ì˜ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì—ì´ì „íŠ¸ ë‹µë³€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                    last_user_question = conversation_history[-2]['content']
                    last_agent_turn = conversation_history[-1]

                    # ì§ì „ ëŒ€í™”ì— ìƒì„¸ ì‹¤í–‰ ë‚´ì—­(steps)ì´ ìˆëŠ”ì§€ í™•ì¸
                    if last_agent_turn['role'] == 'agent' and 'steps' in last_agent_turn and last_agent_turn['steps']:
                        steps_summary = []
                        for step_info in last_agent_turn['steps']:
                            steps_summary.append(
                                f"--- ë‹¨ê³„ {step_info['step']}: {step_info['tool']} ---\n"
                                f"ìš”ì²­: {step_info['description']}\n"
                                f"ê²°ê³¼:\n{step_info['result']}"
                            )
                        detailed_context = "\n\n".join(steps_summary)

                        # ìƒì„¸ ë‚´ì—­ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
                        extraction_prompt = f"""### Instruction:
You are an expert at information extraction. Answer the "User's Follow-up Question" based on the context from the "Previous Conversation". The context includes the detailed step-by-step execution log of how the previous answer was generated.

### Previous Conversation Context:
- Previous User Question: "{last_user_question}"
- Detailed Execution Log from Previous Task:
{detailed_context}

### User's Follow-up Question:
{user_question}

### Extracted Information (Answer directly and concisely in Korean):
"""
                        step_result_data = call_gemini_with_token_tracking(extraction_prompt, "Memory_Retriever_Detailed", file_logger)
                    else:
                        # ìƒì„¸ ì‹¤í–‰ ë‚´ì—­ì´ ì—†ëŠ” ê²½ìš°, ê°„ë‹¨í•œ ë‹µë³€ë§Œ ì°¸ê³ 
                        history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])
                        extraction_prompt = f"""### Instruction:
You are an expert at information extraction. From the 'Full Conversation History', find the specific piece of information needed to answer the 'User's Follow-up Question'.

### Full Conversation History:
{history_str}

### User's Follow-up Question:
{user_question}

### Extracted Information (Answer concisely):
"""
                        step_result_data = call_gemini_with_token_tracking(extraction_prompt, "Memory_Retriever_Simple", file_logger)

                # [ìˆ˜ì •] ì´ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ step_result_dataì— í• ë‹¹í•©ë‹ˆë‹¤.
                # ì´ ê°’ì€ ë£¨í”„ ë§ˆì§€ë§‰ì—ì„œ final_outputì— í• ë‹¹ë˜ì–´ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
                final_output = step_result_data

# elif tool == 'general_qa' ë¸”ë¡ì„ ì•„ë˜ ì½”ë“œë¡œ êµì²´í•©ë‹ˆë‹¤.

            elif tool == 'general_qa':
                print("--- 'general_qa' ë„êµ¬ ì‹¤í–‰: ë…¼ë¬¸ RAG DB ê²€ìƒ‰ ì‹œë„ ---")

                # 1. ë…¼ë¬¸ RAG DBì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ê²€ìƒ‰í•©ë‹ˆë‹¤.
                context, sources = retrieve_paper_context(user_question)
                
                # ì´ ë‹¨ê³„ì˜ ê²°ê³¼ë¬¼ì„ ë‹´ì„ ë³€ìˆ˜
                step_result_data = ""

                # 2. RAG ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬
                if "ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" not in context and context.strip():
                    # [ê²½ë¡œ A] RAG ê²€ìƒ‰ ì„±ê³µ: ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    print(f"âœ… RAG ê²€ìƒ‰ ì„±ê³µ. ì°¸ê³ ë¬¸ì„œ: {sources}")
                    file_logger.info(f"ğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ: {sources}")
                    
                    # RAG ê¸°ë°˜ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ - ì—„ê²©í•œ ê·œì¹™ì—ì„œ ë…¼ë¬¸ì— ì—†ëŠ” ë¶€ë¶„ì€ ì¼ë°˜ì ì¸ LLM ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ ìˆ˜ì •!
                    synthesis_prompt = f"""### Instruction:
You are a helpful AI assistant. Your primary goal is to answer the "User's Question" in Korean.

1.  First, you MUST thoroughly review the "Retrieved Knowledge" provided below. This is your most important reference.
2.  If the "Retrieved Knowledge" contains a direct and complete answer to the "User's Question", base your answer primarily on that information.
3.  If the "Retrieved Knowledge" is relevant but insufficient to fully answer the question, **use it as a starting point or key reference, and supplement it with your general knowledge** to provide a comprehensive and helpful answer.
4.  When you use information directly from the "Retrieved Knowledge", it is good practice to mention that the information is from the provided documents (e.g., "ë…¼ë¬¸ì— ë”°ë¥´ë©´...").

### Retrieved Knowledge:
{context}

### User's Question:
{user_question}

### Final Answer (Korean):
"""
                    step_result_data = call_gemini_with_token_tracking(synthesis_prompt, "General_QA_with_RAG", file_logger)

                else:
                    # [ê²½ë¡œ B] RAG ê²€ìƒ‰ ì‹¤íŒ¨: LLMì˜ ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ (Fallback)
                    print("...RAG ê²€ìƒ‰ ì‹¤íŒ¨. LLM ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                    file_logger.info("ğŸ“„ ë…¼ë¬¸ DBì— ê´€ë ¨ ì •ë³´ê°€ ì—†ì–´, LLM ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ì‹œë„.")

                    # LLMì˜ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ëŠ” ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
                    fallback_prompt = f"""Please answer the following question in Korean based on your general knowledge.
Question: "{user_question}"
Answer (Korean):"""
                    step_result_data = call_gemini_with_token_tracking(fallback_prompt, "General_QA_Fallback", file_logger)

            elif tool == 'report_generator':
                # ì´ ë„êµ¬ëŠ” í•­ìƒ ë§ˆì§€ë§‰ì— ì‹¤í–‰ë˜ì–´ì•¼ í•¨
                # ì´ì „ ë‹¨ê³„ë“¤ì˜ ê²°ê³¼ê°€ ë‹´ê¸´ execution_contextë¥¼ ì‚¬ìš©.
                report_content = generate_report(user_question, execution_context, conversation_history, plan)
                step_result_data = report_content

            # --- ì¤‘ê°„ ê²°ê³¼ ì €ì¥ --- 
            # [í•µì‹¬ ìˆ˜ì •] ê° ë‹¨ê³„ì˜ ê²°ê³¼(step_result_data)ë¥¼ ë‹¤ìŒ ìµœì¢… ì¢…í•© ë‹¨ê³„ë¥¼ ìœ„í•´ execution_contextì— ì°¨ê³¡ì°¨ê³¡ ì €ì¥í•©ë‹ˆë‹¤.
            execution_context[f'output_of_step_{i+1}'] = step_result_data
            
            # ì‚¬ìš©ìì™€ ë¡œê·¸ì— ë³´ì—¬ì¤„ ì¤‘ê°„ ìš”ì•½ë³¸ ìƒì„±
            if isinstance(step_result_data, pd.DataFrame):
                if step_result_data.empty:
                    final_output = "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                else:
                    final_output = f"ë°ì´í„° ì¡°íšŒë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. {len(step_result_data)}ê°œì˜ í–‰ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n(ë¯¸ë¦¬ë³´ê¸°)\n{step_result_data.head().to_string()}"
            else: # DataFrameì´ ì•„ë‹Œ ê²½ìš° (í…ìŠ¤íŠ¸ ë“±)
                final_output = str(step_result_data)

            step_log_entry = {
                "step": i + 1, "tool": tool, "description": description,
                "result": final_output[:500] + ("..." if len(final_output) > 500 else "")
            }
            intermediate_outputs.append(step_log_entry)

            if len(plan) > 1 and tool != plan[-1]['tool']: # ë§ˆì§€ë§‰ ë‹¨ê³„ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ì¤‘ê°„ ê²°ê³¼ ì¶œë ¥
                print(f"[ì¤‘ê°„ ê²°ê³¼ {i+1}] {final_output[:100]}...")
            
            file_logger.info(f"ğŸ“Š {i+1}ë‹¨ê³„ ê²°ê³¼:\n---\n{final_output}\n---")

        # ì¶”ê°€ 07-04 13:03
        # [NEW] for ë£¨í”„ê°€ ëë‚œ í›„, ê³„íšì— answer_synthesizerê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¤í–‰
        final_synthesis_step = next((step for step in plan if step['tool'] == 'answer_synthesizer'), None)

        if final_synthesis_step:
            print(f"--- ìµœì¢… ë‹¨ê³„ ì‹¤í–‰: answer_synthesizer ({final_synthesis_step['description']}) ---")

            # execution_contextì— ì €ì¥ëœ ëª¨ë“  ì¤‘ê°„ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
            combined_context = []
            # ë£¨í”„ëŠ” synthesizerë¥¼ ì œì™¸í•œ ë‹¨ê³„ê¹Œì§€ë§Œ ë•ë‹ˆë‹¤.
            for i in range(len(plan) - 1):
                step_key = f"output_of_step_{i+1}"
                step_plan = plan[i]
                
                if step_key in execution_context:
                    step_output = execution_context[step_key]
                    header = f"### [ê²°ê³¼ from Step {i+1}: {step_plan['tool']} - {step_plan['description']}]"
                    
                    if isinstance(step_output, pd.DataFrame):
                        if step_output.empty:
                            body = "ê²°ê³¼ ì—†ìŒ."
                        else:
                            body = step_output.to_markdown(index=False)
                    else:
                        body = str(step_output)
                        
                    combined_context.append(f"{header}\n{body}")
            
            retrieved_data = "\n\n---\n\n".join(combined_context)
            
            # ì‚¬ìš©ìë‹˜ì´ ì™„ì„±í•œ ê°•ë ¥í•œ ìµœì¢… ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ê¸°ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
            history_str = "\n".join([f"## {msg['role']}:\n{msg['content']}" for msg in conversation_history])
            synthesis_prompt = f"""### Persona and Goal
You are a professional AI data analyst for a data center. Your goal is to provide a final, insightful, and user-friendly answer in Korean based on the provided information.

### Contextual Information Provided
1.  **User's Current Question**: The user's most recent query.
2.  **Data From Previous Step**: A collection of results from all preceding steps. This can be empty.
3.  **Operational Policy**: The official operational standards for the data center.
4.  **Conversation History**: The log of the conversation so far.

### Step-by-Step Instructions
1.  Thoroughly analyze the `User's Current Question` to understand their primary intent.
2.  Examine the `Data From Previous Step`. This contains all the information gathered so far.
3.  **[CRITICAL] How to Handle Empty Data**:
    - If the `Data From Previous Step` is empty or contains "ê²°ê³¼ ì—†ìŒ", DO NOT just say "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    - Instead, look at the `User's Current Question`. If the question was asking *if something exists* (e.g., "any alarms?"), an empty result definitively means "no, none exist." State this clearly.
    - Example: If the question was "Are there any active alarms?" and the data is empty, your answer MUST be "í˜„ì¬ í™œì„±í™”ëœ ì•ŒëŒì´ ì—†ìŠµë‹ˆë‹¤."
4.  **How to Handle Available Data**:
    - If data exists, synthesize all pieces of information into a coherent answer.
    - First, state the key facts from the data (e.g., "The current temperature is 25Â°C.").
    - Next, compare these facts with the `Operational Policy`.
    - Make a clear judgment based on the policy (e.g., "This is within the normal range.").
5.  Formulate a final, concise, and insightful answer in Korean.

---
### Provided Information

#### [User's Current Question]
{user_question}

#### [Data From Previous Step]
{retrieved_data}
#### [Operational Policy]
- **ì˜¨ë„ (Temperature)**: 18Â°C ~ 27Â°C (ì •ìƒ), 27Â°C ì´ˆê³¼ (ì£¼ì˜), 32Â°C ì´ˆê³¼ (ê²½ê³ )
- **ìŠµë„ (Humidity)**: 40% ~ 60% (ì •ìƒ)
- **PUE**: 1.2 ì´í•˜ (ë§¤ìš° ìš°ìˆ˜), 1.3~1.6 ì´í•˜ (ìš°ìˆ˜), 1.6~2.0 ì´í•˜ (ë³´í†µ), 2.0 ì´ìƒ (ì—ë„ˆì§€ íš¨ìœ¨ ë‚®ìŒ)
- **ì•ŒëŒ(Alarm)**: ê°’ì´ 1ì´ë©´ ì¦‰ì‹œ í™•ì¸ í•„ìš”

#### [Conversation History]
{history_str}
---

### Final Answer (in Korean):
"""
            final_output = call_gemini_with_token_tracking(synthesis_prompt, "Final_Answer_Synthesis", file_logger)

    except Exception as e:
        final_output = f"ğŸ’¥ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        file_logger.error(final_output, exc_info=True)

    # --- ëŒ€í™” ê¸°ë¡ ì €ì¥ ë° ìµœì¢… ì¶œë ¥ ---
    conversation_history.append({"role": "user", "content": user_question})
    conversation_history.append({"role": "agent", "content": final_output, "steps": intermediate_outputs})
    
    MAX_HISTORY_TURNS = 5
    if len(conversation_history) > MAX_HISTORY_TURNS * 2:
        conversation_history = conversation_history[-(MAX_HISTORY_TURNS * 2):]

    print(f"\n[ğŸ¤–ì—ì´ì „íŠ¸ ë‹µë³€]\n{final_output}\n")
    file_logger.info("="*40 + "\n")

    # 0710 eval ì½”ë“œ ë•Œë¬¸ì— ì¶”ê°€
    return final_output

# --- 4. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì§€ì  ---
if __name__ == "__main__":
    file_logger = setup_file_logger()
    conversation_history = []
    print("ğŸ¤– LLM ì—ì´ì „íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit' ì…ë ¥)")
    
    while True:
        try:
            user_input = input("ğŸ§‘ë‹¹ì‹ : ")
            if user_input.lower() in ["exit", "quit"]:

                # [NEW] ì¢…ë£Œ ì‹œ ì´ í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥
                print("\n" + "="*40)
                print("ğŸ“Š ì„¸ì…˜ ì´ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½")
                print(f" - ì´ ì…ë ¥ í† í°: {token_tracker['total_prompt_tokens']}")
                print(f" - ì´ ì¶œë ¥ í† í°: {token_tracker['total_completion_tokens']}")
                print(f" - í•©ê³„ í† í°: {token_tracker['total_tokens']}")
                print("="*40)
                print("ğŸ‘‹ ì—ì´ì „íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not user_input.strip():  # ë¹ˆ ì…ë ¥ì€ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì…ë ¥ì„ ë°›ìŒ(ì‹¤ìˆ˜ë¡œ ì—”í„°ë§Œ ì³¤ì„ ë•Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•¨)
                continue
            run_ai_agent(user_input, file_logger, conversation_history)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì—ì´ì „íŠ¸ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            error_msg = f"ğŸ’¥ ë©”ì¸ ë£¨í”„ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"
            print(error_msg)
            file_logger.error(error_msg, exc_info=True)